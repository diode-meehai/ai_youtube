{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    @classmethod\n",
    "    def unique_value(self, X):\n",
    "        unique_value = np.unique(X)\n",
    "        return unique_value\n",
    "    \n",
    "    @classmethod\n",
    "    def one_hot_matrix(self, X, unique_value):\n",
    "        N = X.shape[0]\n",
    "        n_value = len(unique_value)\n",
    "        one_hot_matrix = np.zeros([N, n_value])\n",
    "        for i, x in enumerate(X[:, 0:1]):\n",
    "            j = np.argwhere(unique_value == x[0]).ravel()[0]\n",
    "            one_hot_matrix[i, j] = 1\n",
    "        return one_hot_matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def create_one_hot_matrix(self, X):\n",
    "        unique_value = self.unique_value(X)\n",
    "        one_hot_matrix = self.one_hot_matrix(X, unique_value)\n",
    "        return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorScore:\n",
    "    @classmethod\n",
    "    def find_error(self, Y, Yhat, error_type):\n",
    "        if error_type == 'SSE':\n",
    "            error = self.find_SSE(Y, Yhat)\n",
    "        elif error_type == 'MSE':\n",
    "            error = self.find_MSE(Y, Yhat)\n",
    "        elif error_type == 'MAE':\n",
    "            error = self.find_MAE(Y, Yhat)\n",
    "        elif error_type == 'MAPE':\n",
    "            error = self.find_MAPE(Y, Yhat)\n",
    "        elif error_type == 'entropy':\n",
    "            error = self.find_entropy(Y, Yhat)\n",
    "        elif error_type == 'binary':\n",
    "            error = self.find_error_bin_class(Y, Yhat)\n",
    "        elif error_type == 'multiclass':\n",
    "            error = self.find_error_mul_class(Y, Yhat)\n",
    "        return error\n",
    "    \n",
    "    @classmethod\n",
    "    def find_SSE(self, Y, Yhat):\n",
    "        SSE = ((Y - Yhat)**2).sum()\n",
    "        return SSE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_MSE(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        SSE = ((Y - Yhat)**2).sum()\n",
    "        MSE = SSE/N\n",
    "        return MSE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_MAE(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        MAE = (np.abs(Y - Yhat)).sum()/N\n",
    "        return MAE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_MAPE(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        MAPE = np.abs((Y - Yhat)/Y).sum()*100/N\n",
    "        return MAPE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_entropy(self, Y, Yhat):\n",
    "        entropy = (-Y*np.log(Yhat)).sum()\n",
    "        return entropy\n",
    "    \n",
    "    @classmethod\n",
    "    def find_error_bin_class(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        _Y = np.round(Y, 0)\n",
    "        _Yhat = np.round(Yhat, 0)\n",
    "        error = 100*(_Y != _Yhat).sum()/N\n",
    "        return error\n",
    "    \n",
    "    @classmethod\n",
    "    def find_error_mul_class(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        Y_argmax = np.argmax(Y, axis=1)\n",
    "        Yhat_argmax = np.argmax(Yhat, axis=1)\n",
    "        error = 100*(Y_argmax != Yhat_argmax).sum()/N\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    @classmethod\n",
    "    def r(self, X, Y, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        if X.shape[1] != 1:\n",
    "            raise Exception('n_features must be 1')\n",
    "        if Y.shape[1] != 1:\n",
    "            raise Exception('n_targets must be 1')\n",
    "            \n",
    "        if sample_weight is None:\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "        else:\n",
    "            X = sample_weight*X\n",
    "            Y = sample_weight*Y\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "        fraction = ((X - X_mean)*(Y - Y_mean)).sum()\n",
    "        denominator = np.sqrt(((X- X_mean)**2).sum()*((Y - Y_mean)**2).sum())\n",
    "        r = fraction/denominator\n",
    "        return r\n",
    "            \n",
    "    def fit(self, X, Y, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        if X.shape[1] != 1:\n",
    "            raise Exception('n_features must be 1')\n",
    "        if Y.shape[1] != 1:\n",
    "            raise Exception('n_targets must be 1')\n",
    "            \n",
    "        XY = X*Y\n",
    "        X2 = X**2\n",
    "        if sample_weight is None:\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "            XY_mean = XY.mean()\n",
    "            X2_mean = X2.mean()\n",
    "        else:\n",
    "            X = sample_weight*X\n",
    "            Y = sample_weight*Y\n",
    "            XY = sample_weight*XY\n",
    "            X2 = sample_weight*X2\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "            XY_mean = XY.mean()\n",
    "            X2_mean = X2.mean()\n",
    "        denominator = X2_mean - X_mean**2\n",
    "        self.a = (XY_mean - X_mean*Y_mean)/denominator\n",
    "        self.b = (X2_mean*Y_mean - X_mean*XY_mean)/denominator\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Yhat = self.a*X + self.b\n",
    "        return Yhat\n",
    "    \n",
    "    def scatter(self, X, Y, line=False):\n",
    "        plt.scatter(X, Y)\n",
    "        plt.plot(X, self.a*X + self.b, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleRegression(ErrorScore):\n",
    "    def __init__(self, approach='Global', n_jobs=None):\n",
    "        self.approach = approach\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    def fit(self, X, Y, epoch=1000, learning_rate=0.01, error_type='SSE', sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        if Y.shape[1] != 1:\n",
    "            raise Exception('n_targets must be 1')\n",
    "        N = X.shape[0]\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        if self.approach == 'Global':\n",
    "            front = inv(np.dot(Xb.T, Xb))\n",
    "            back = np.dot(Xb.T, Y)\n",
    "            self.W = np.dot(front, back)\n",
    "        elif self.approach == 'Local':\n",
    "            error_list = []\n",
    "            D = Xb.shape[1]\n",
    "            W = np.random.randn(D, 1)\n",
    "            for i in range(epoch):\n",
    "                Yhat = np.dot(Xb, W)\n",
    "                error = ErrorScore.find_error(Y, Yhat, error_type)\n",
    "                error_list.append(error)\n",
    "                W = W + (learning_rate/N)*np.dot(Xb.T, Y-Yhat)\n",
    "            self.error_list = error_list\n",
    "            self.W = W\n",
    "    \n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        Yhat = np.dot(Xb, self.W)\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    @classmethod\n",
    "    def sigmoid(self, Z):\n",
    "        # Yhat = 1/(1 + np.e**(-Z))\n",
    "        Yhat = 1/(1 + np.exp(-Z)) #faster\n",
    "        return Yhat\n",
    "    \n",
    "    @classmethod\n",
    "    def softmax(self, Z):\n",
    "        Yhat = np.exp(Z)/np.exp(Z).sum(axis=1, keepdims = True)\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(ActivationFunction, ErrorScore):\n",
    "    def __init__(self, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    def fit(self, X, Y, epoch=1000, learning_rate=0.01, error_type='entropy', sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        N = X.shape[0]\n",
    "        D = X.shape[1] + 1\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        K = Y.shape[1]\n",
    "        W = np.random.randn(D, K)\n",
    "        error_list = []\n",
    "        for i in range(epoch):\n",
    "            Z = np.dot(Xb, W)\n",
    "            if K == 1:\n",
    "                Yhat = ActivationFunction.sigmoid(Z)\n",
    "            elif K > 1:\n",
    "                Yhat = ActivationFunction.softmax(Z)\n",
    "            \n",
    "            error = ErrorScore.find_error(Y, Yhat, error_type)\n",
    "            error_list.append(error)\n",
    "            W = W + (learning_rate/N)*np.dot(Xb.T, Y-Yhat)\n",
    "        self.error_list = error_list\n",
    "        self.W = W\n",
    "    \n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        Z = np.dot(Xb, self.W)\n",
    "        K = self.W.shape[1]\n",
    "        if K == 1:\n",
    "            Yhat = ActivationFunction.sigmoid(Z)\n",
    "        elif K > 1:\n",
    "            Yhat = ActivationFunction.softmax(Z)\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    adj = []\n",
    "    total_id = 0\n",
    "    e = 2.718281828459045\n",
    "    n_samples = 'wait for assigned'\n",
    "    \n",
    "    def __init__(self, data, represent, torch_type='sample'):\n",
    "        _dict = {}\n",
    "        _dict['id'] = tensor.total_id\n",
    "        _dict['data'] = data\n",
    "        _dict['represent'] = represent\n",
    "        _dict['torch_type'] = torch_type\n",
    "        _dict['created_from'] = 'assigned'\n",
    "        _dict['send_to'] = []\n",
    "        _dict['diff'] = []\n",
    "        _dict['family'] = set()\n",
    "        _dict['storage_chain_rule'] = None\n",
    "        if torch_type == 'sample' and tensor.n_samples == 'wait for assigned':\n",
    "            tensor.n_samples = represent.shape[0]\n",
    "        tensor.adj.append(_dict)\n",
    "        self.id = tensor.total_id\n",
    "        tensor.total_id += 1\n",
    "        self.data = data\n",
    "        \n",
    "    def update_so(self, self_id, other_id, new_var_id, operation):\n",
    "        tensor.adj[self_id]['send_to'].append(new_var_id)\n",
    "        tensor.adj[other_id]['send_to'].append(new_var_id)\n",
    "        tensor.adj[new_var_id]['created_from'] = operation\n",
    "        \n",
    "        self_set = tensor.adj[self_id]['family']\n",
    "        other_set = tensor.adj[other_id]['family']\n",
    "        tensor.adj[new_var_id]['family'].add(self_id)\n",
    "        tensor.adj[new_var_id]['family'].add(other_id)\n",
    "        tensor.adj[new_var_id]['family'] = tensor.adj[new_var_id]['family'] | self_set | other_set\n",
    "        \n",
    "    def update_s(self, self_id, new_var_id, operation):\n",
    "        tensor.adj[self_id]['send_to'].append(new_var_id)\n",
    "        tensor.adj[new_var_id]['created_from'] = operation\n",
    "        \n",
    "        self_set = tensor.adj[self_id]['family']\n",
    "        tensor.adj[new_var_id]['family'].add(self_id)\n",
    "        tensor.adj[new_var_id]['family'] = tensor.adj[new_var_id]['family'] | self_set\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent'] + tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data + other.data, represent)\n",
    "            operation = ('+', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "            tensor.adj[other.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent'] + other\n",
    "            new_var = tensor(self.data + other, represent)\n",
    "            operation = ('+', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other + tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other + self.data, represent)\n",
    "            operation = ('+', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent'] - tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data - other.data, represent)\n",
    "            operation = ('-', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "            tensor.adj[other.id]['diff'].append(-np.ones([tensor.n_samples, 1]))\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent'] - other\n",
    "            new_var = tensor(self.data - other, represent)\n",
    "            operation = ('-', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other - tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other - self.data, represent)\n",
    "            operation = ('-', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(-np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent']*tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data * other.data, represent)\n",
    "            operation = ('*', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[other.id]['diff'].append(tensor.adj[self.id]['represent'])\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent']*other\n",
    "            new_var = tensor(self.data * other, represent)\n",
    "            operation = ('*', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(other * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other*tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other * self.data, represent)\n",
    "            operation = ('*', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(other * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent']/tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data / other.data, represent)\n",
    "            operation = ('/', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(1/tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[other.id]['diff'].append(-tensor.adj[self.id]['represent']/tensor.adj[other.id]['represent']**2)\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent']/other\n",
    "            new_var = tensor(self.data / other, represent)\n",
    "            operation = ('/', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append((1/other) * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other / tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other / self.data, represent)\n",
    "            operation = ('/', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(-other/tensor.adj[self.id]['represent']**2)\n",
    "        return new_var\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent'] ** tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data ** other.data, represent)\n",
    "            operation = ('**', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            self_rep = tensor.adj[self.id]['represent']\n",
    "            other_rep = tensor.adj[other.id]['represent']\n",
    "            tensor.adj[self.id]['diff'].append(other_rep*self_rep**(other_rep-1))\n",
    "            tensor.adj[other.id]['diff'].append((self_rep**other_rep)*np.log(self_rep))\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent'] ** other\n",
    "            new_var = tensor(self.data ** other, represent)\n",
    "            operation = ('**', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            self_rep = tensor.adj[self.id]['represent']\n",
    "            tensor.adj[self.id]['diff'].append(other*self_rep**(other-1))\n",
    "        return new_var\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other ** tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other ** self.data, represent)\n",
    "            operation = ('**', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            self_rep = tensor.adj[self.id]['represent']\n",
    "            tensor.adj[self.id]['diff'].append((other**self_rep)*np.log(other))\n",
    "        return new_var\n",
    "    \n",
    "    def __neg__(self):\n",
    "        represent = -tensor.adj[self.id]['represent']\n",
    "        new_var = tensor(-self.data, represent)\n",
    "        operation = ('-1*', self.id)\n",
    "        self.update_s(self.id, new_var.id, operation)\n",
    "        tensor.adj[self.id]['diff'].append(-np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __abs__(self):\n",
    "        represent = np.abs(tensor.adj[self.id]['represent'])\n",
    "        new_var = tensor(abs(self.data), represent)\n",
    "        operation = ('abs', self.id)\n",
    "        self.update_s(self.id, new_var.id, operation)\n",
    "        diff_abs = (tensor.adj[self.id]['represent'] > 0) -1*(tensor.adj[self.id]['represent'] < 0)\n",
    "        tensor.adj[self.id]['diff'].append(diff_abs * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def log(self):\n",
    "        represent = np.log(tensor.adj[self.id]['represent'])\n",
    "        new_var = tensor(np.log(self.data), represent)\n",
    "        operation =  ('log', self.id)\n",
    "        self.update_s(self.id, new_var.id, operation)\n",
    "        tensor.adj[self.id]['diff'].append(1/(tensor.adj[self.id]['represent']))\n",
    "        return new_var\n",
    "    \n",
    "    def maximum(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = np.maximum(tensor.adj[self.id]['represent'], tenor.adj[other.id]['represent'])\n",
    "            new_var = tensor(np.maximum(self.data, other.data), represent)\n",
    "            operation = ('maximum', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensnor.adj[self.id]['represent'])\n",
    "            diff_other = (represent == tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "            tensor.adj[other.id]['diff'].append(diff_other)\n",
    "        else:\n",
    "            represent = np.maximum(tensor.adj[self.id]['represent'], other)\n",
    "            new_var = tensor(np.maximum(self.data, other), represent)\n",
    "            operation = ('maximum', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensor.adj[self.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "        return new_var\n",
    "    \n",
    "    def minimum(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = np.minimum(tensor.adj[self.id]['represent'], tenor.adj[other.id]['represent'])\n",
    "            new_var = tensor(np.minimum(self.data, other.data), represent)\n",
    "            operation = ('minimum', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensnor.adj[self.id]['represent'])\n",
    "            diff_other = (represent == tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "            tensor.adj[other.id]['diff'].append(diff_other)\n",
    "        else:\n",
    "            represent = np.minimum(tensor.adj[self.id]['represent'], other)\n",
    "            new_var = tensor(np.minimum(self.data, other), represent)\n",
    "            operation = ('minimum', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensor.adj[self.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "        return new_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch(tensor):\n",
    "    e = 2.718281828459045\n",
    "    n_samples = 'wait for assigned'\n",
    "    \n",
    "    def __init__(self, data, torch_type='sample', assigned=False):\n",
    "        self.data = data\n",
    "        self.torch_type = torch_type\n",
    "        if assigned == True:\n",
    "            if torch_type == 'sample':\n",
    "                self.rep = self.ele_to_tensor(data, torch_type)\n",
    "                if torch.n_samples == 'wait for assigned':\n",
    "                    torch.n_samples = data.shape[0]\n",
    "            elif torch_type == 'weight':\n",
    "                self.rep = self.ele_to_tensor(data, torch_type)\n",
    "            elif torch_type == 'bias':\n",
    "                self.rep = self.ele_to_tensor(data, torch_type)\n",
    "        else:\n",
    "            self.rep = 'wait for assigned'\n",
    "\n",
    "    def ele_to_tensor(self, matrix, torch_type):\n",
    "        if torch_type == 'sample':\n",
    "            sample_matrix = matrix[:1, :]\n",
    "            o_matrix = np.array(sample_matrix, dtype='object')\n",
    "        elif torch_type != 'sample':\n",
    "            o_matrix = np.array(matrix, dtype='object')\n",
    "        n_rows, n_cols = o_matrix.shape\n",
    "        for r in range(n_rows):\n",
    "            for c in range(n_cols):\n",
    "                if torch_type == 'sample':\n",
    "                    o_matrix[r,c] = tensor(o_matrix[r,c], matrix[:,c:c+1], torch_type)\n",
    "                elif torch_type != 'sample':\n",
    "                    o_matrix[r,c] = tensor(o_matrix[r,c], \n",
    "                                    o_matrix[r,c]*np.ones([torch.n_samples, 1]), torch_type)\n",
    "        return o_matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def ele_to_numeric(self, matrix):\n",
    "        matrix = np.array(matrix, dtype='object')\n",
    "        n_rows, n_cols = matrix.shape\n",
    "        for r in range(n_rows):\n",
    "            for c in range(n_cols):\n",
    "                matrix[r,c] = matrix[r,c].data\n",
    "        return matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def clear_adj(self):\n",
    "        tensor.adj = []\n",
    "        tensor.total_id = 0\n",
    "        tensor.n_samples = 'wait for assigned'\n",
    "        torch.n_samples = 'wait for assigned'\n",
    "            \n",
    "    def dot(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            new_var = torch(np.dot(self.data, other.data))\n",
    "            new_var.rep = np.dot(self.rep, other.rep)\n",
    "        else:\n",
    "            new_var = torch(np.dot(self.data, other))\n",
    "            new_var.rep = np.dot(self.rep, other)\n",
    "        return new_var\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data + other.data)\n",
    "            new_var.rep = self.rep + other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data + other)\n",
    "            new_var.rep = self.rep + other\n",
    "        return new_var\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other + self.data)\n",
    "            new_var.rep = other + self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data - other.data)\n",
    "            new_var.rep = self.rep - other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data - other)\n",
    "            new_var.rep = self.rep - other\n",
    "        return new_var\n",
    "            \n",
    "    def __rsub__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other - self.data)\n",
    "            new_var.rep = other - self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data * other.data)\n",
    "            new_var.rep = self.rep * other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data * other)\n",
    "            new_var.rep = self.rep * other\n",
    "        return new_var\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other * self.data)\n",
    "            new_var.rep = other * self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data / other.data)\n",
    "            new_var.rep = self.rep / other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data / other)\n",
    "            new_var.rep = self.rep / other\n",
    "        return new_var\n",
    "        \n",
    "    def __rtruediv__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other / self.data)\n",
    "            new_var.rep = other / self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data ** other.data)\n",
    "            new_var.rep = self.rep ** other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data ** other)\n",
    "            new_var.rep = self.rep ** other\n",
    "        return new_var\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other ** self.data)\n",
    "            new_var.rep = other ** self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __neg__(self):\n",
    "        new_var = torch(-self.data)\n",
    "        new_var.rep = -self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def abs(self):\n",
    "        new_var = torch(np.abs(self.data))\n",
    "        new_var.rep = np.abs(self.rep)\n",
    "        return new_var\n",
    "    \n",
    "    def log(self):\n",
    "        new_var = torch(np.log(self.data))\n",
    "        new_var.rep = np.log(self.rep)\n",
    "        return new_var\n",
    "\n",
    "    def maximum(self, other):\n",
    "        if isinstance(self, torch):\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.maximum(self.data, other.data))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                max_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        max_tensor = tensor.maximum(self.rep[r,c], other.rep[r,c])\n",
    "                        max_matrix[r,c] = max_tensor\n",
    "                new_var.rep = max_matrix\n",
    "            else:\n",
    "                new_var = torch(np.maximum(self.data, other))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                max_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        max_tensor = tensor.maximum(self.rep[r,c], other)\n",
    "                        max_matrix[r,c] = max_tensor\n",
    "                new_var.rep = max_matrix\n",
    "        else:\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.maximum(self, other.data))\n",
    "                n_rows, n_cols = other.rep.shape\n",
    "                max_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        max_tensor = tensor.maximum(other.rep[r,c], self)\n",
    "                        max_matrix[r,c] = max_tensor\n",
    "                new_var.rep = max_matrix\n",
    "            else:\n",
    "                new_var = torch(np.maximum(self, other))\n",
    "        return new_var\n",
    "    \n",
    "    def minimum(self, other):\n",
    "        if isinstance(self, torch):\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.minimum(self.data, other.data))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                min_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        min_tensor = tensor.minimum(self.rep[r,c], other.rep[r,c])\n",
    "                        min_matrix[r,c] = min_tensor\n",
    "                new_var.rep = min_matrix\n",
    "            else:\n",
    "                new_var = torch(np.minimum(self.data, other))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                min_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        min_tensor = tensor.minimum(self.rep[r,c], other)\n",
    "                        min_matrix[r,c] = min_tensor\n",
    "                new_var.rep = min_matrix\n",
    "        else:\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.minimum(self, other.data))\n",
    "                n_rows, n_cols = other.rep.shape\n",
    "                min_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        min_tensor = tensor.minimum(other.rep[r,c], self)\n",
    "                        min_matrix[r,c] = min_tensor\n",
    "                new_var.rep = min_matrix\n",
    "            else:\n",
    "                new_var = torch(np.maximum(self, other))\n",
    "        return new_var\n",
    "\n",
    "    def relu(self):\n",
    "        relu = torch.maximum(self, 0)\n",
    "        return relu\n",
    "    \n",
    "    def prelu(self, alpha):\n",
    "        prelu = torch.maximum(self, 0) + alpha*torch.minimum(self, 0)\n",
    "        return prelu\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        sigmoid = 1/(1 + torch.e**(-self))\n",
    "        return sigmoid\n",
    "    \n",
    "    def tanh(self):\n",
    "        tanh = (torch.e**self - torch.e**(-self))/(torch.e**self + torch.e**(-self))\n",
    "        return tanh\n",
    "    \n",
    "    def softmax(self):\n",
    "        eZ = torch.e**self\n",
    "        denominator = torch.col_sum(eZ)\n",
    "        softmax = eZ/denominator\n",
    "        return softmax\n",
    "    \n",
    "    def col_sum(self):\n",
    "        new_var = torch(np.sum(self.data, axis=1, keepdims=True))\n",
    "        new_var.rep = np.sum(self.rep, axis=1, keepdims=True)\n",
    "        return new_var\n",
    "    \n",
    "    def sum(self):\n",
    "        new_var = torch(np.sum(self.data))\n",
    "        new_var.rep = np.sum(self.rep)\n",
    "        return new_var\n",
    "    \n",
    "    def mean(self):\n",
    "        new_var = torch(np.mean(self.data))\n",
    "        # new_var.rep = np.mean(self.rep)\n",
    "        new_var.rep = self.rep/tensor.n_samples\n",
    "        return new_var\n",
    "    \n",
    "    def sse(self, other):\n",
    "        error = (self - other)**2\n",
    "        sse = torch.sum(error)\n",
    "        return sse\n",
    "    \n",
    "    def mse(self, other):\n",
    "        error = (self - other)**2\n",
    "        mse = torch.mean(error)\n",
    "        return mse\n",
    "    \n",
    "    def m2_entropy(self, other):\n",
    "        error = -(self*np.log(other+np.finfo(np.float32).eps) + (1-self)*np.log(1-other+np.finfo(np.float32).eps))\n",
    "        entropy = torch.mean(error)\n",
    "        return entropy\n",
    "    \n",
    "    def m_entropy(self, other):\n",
    "        error = self*np.log(other+np.finfo(np.float32).eps)\n",
    "        entropy = torch.mean(error)\n",
    "        return entropy\n",
    "    \n",
    "    @classmethod\n",
    "    def forward_diff(self, y_id, x_id):\n",
    "        if (x_id in tensor.adj[y_id]['family']) or (x_id == y_id):\n",
    "            if tensor.adj[x_id]['storage_chain_rule'] is not None:\n",
    "                return tensor.adj[x_id]['storage_chain_rule']\n",
    "            else:\n",
    "                send_to = tensor.adj[x_id]['send_to']\n",
    "                n_send = len(send_to)\n",
    "                if n_send == 1:\n",
    "                    target_id = send_to[0]\n",
    "                    dy_dx = tensor.adj[x_id]['diff'][0] * self.forward_diff(y_id, target_id)\n",
    "                    tensor.adj[x_id]['storage_chain_rule'] = dy_dx\n",
    "                    return dy_dx\n",
    "                elif n_send > 1:\n",
    "                    dy_dx = 0\n",
    "                    for i in range(n_send):\n",
    "                        target_id = send_to[i]\n",
    "                        dy_dx += tensor.adj[x_id]['diff'][i] * self.forward_diff(y_id, target_id)\n",
    "                    tensor.adj[x_id]['storage_chain_rule'] = dy_dx\n",
    "                    return dy_dx\n",
    "                elif n_send == 0:\n",
    "                    return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def gradient(self):\n",
    "        n_rows, n_cols = self.rep.shape\n",
    "        diff_matrix = np.zeros([n_rows, n_cols])\n",
    "        for r in range(n_rows):\n",
    "            for c in range(n_cols):\n",
    "                diff_matrix[r,c] = np.sum(torch.forward_diff(tensor.adj[-1]['id'], self.rep[r,c].id))\n",
    "        return diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch):\n",
    "    def __init__(self, HL, AF, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "        if len(HL) != len(AF):\n",
    "            raise Exception('n_layers must be same')\n",
    "        self.HL = HL\n",
    "        self.AF = AF\n",
    "        \n",
    "    def fit(self, X, Y, loss_function, epoch=1000, learning_rate=0.01, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_sample must be same')\n",
    "        D = X.shape[1]\n",
    "        W, B = self.create_WB(D, self.HL)\n",
    "        loss_list = []\n",
    "        for i in range(epoch):\n",
    "            t_X0 = torch(X, torch_type='sample', assigned=True)\n",
    "            t_Y = torch(Y, torch_type='sample', assigned=True)\n",
    "            t_W = self.to_torch(W, torch_type='weight')\n",
    "            t_B = self.to_torch(B, torch_type='bias')\n",
    "            t_Z, t_X = self.forward(t_X0, t_W, t_B, self.AF)\n",
    "            loss = self.compute_loss(t_Y, t_X[-1], loss_function)\n",
    "            loss_list.append(loss.data)\n",
    "            dW, dB = self.compute_gradient(t_W, t_B)\n",
    "            W, B = self.update_WB(W, B, learning_rate, dW, dB)\n",
    "            torch.clear_adj()\n",
    "        self.loss_list = loss_list\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "            \n",
    "    def create_WB(self, D, HL):\n",
    "        W = []\n",
    "        B = []\n",
    "        for i in range(len(HL)):\n",
    "            if i == 0:\n",
    "                W_i = np.random.randn(D, HL[0])/np.sqrt(HL[0])\n",
    "            else:\n",
    "                W_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "            B_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            W.append(W_i)\n",
    "            B.append(B_i)\n",
    "        return W, B\n",
    "    \n",
    "    def to_torch(self, tensor_list, torch_type, assigned=True):\n",
    "        torch_list = []\n",
    "        for i in range(len(tensor_list)):\n",
    "            _torch = torch(tensor_list[i], torch_type, assigned)\n",
    "            torch_list.append(_torch)\n",
    "        return torch_list\n",
    "    \n",
    "    def forward(self, t_X0, t_W, t_B, AF):\n",
    "        n_layers = len(t_W)\n",
    "        Z = []\n",
    "        A = []\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                Z_i = torch.dot(t_X0, t_W[0]) + t_B[0]\n",
    "            else:\n",
    "                Z_i = torch.dot(A_i, t_W[i]) + t_B[i]\n",
    "            A_i = self.compute_A(Z_i, AF[i])\n",
    "            Z.append(Z_i)\n",
    "            A.append(A_i)\n",
    "        return Z, A\n",
    "    \n",
    "    def compute_A(self, Z_i, af):\n",
    "        if isinstance(af, str):\n",
    "            if af == 'sigmoid':\n",
    "                A_i = torch.sigmoid(Z_i)\n",
    "            elif af == 'tanh':\n",
    "                A_i = torch.tanh(Z_i)\n",
    "            elif af == 'relu':\n",
    "                A_i = torch.relu(Z_i)\n",
    "            elif af == 'softmax':\n",
    "                A_i = torch.softmax(Z_i)\n",
    "        elif isinstance(af, list):\n",
    "            if af[0] == 'prelu':\n",
    "                A_i = torch.prelu(Z_i, af[1])\n",
    "        return A_i\n",
    "        \n",
    "    def compute_loss(self, t_Y, t_Yhat, loss_function):\n",
    "        if loss_function == 'sse':\n",
    "            loss = torch.sse(t_Y, t_Yhat)\n",
    "        elif loss_function == 'mse':\n",
    "            loss = torch.mse(t_Y, t_Yhat)\n",
    "        elif loss_function == 'm2_entropy':\n",
    "            loss = torch.m2_entropy(t_Y, t_Yhat)\n",
    "        elif loss_function == 'm_entropy':\n",
    "            loss = torch.m_entropy(t_Y, t_Yhat)\n",
    "        return loss\n",
    "        \n",
    "    def compute_gradient(self, t_W, t_B):\n",
    "        n_layers = len(t_W)\n",
    "        dW = []\n",
    "        dB = []\n",
    "        for i in range(n_layers):\n",
    "            dW_i = torch.gradient(t_W[i])\n",
    "            dB_i = torch.gradient(t_B[i])\n",
    "            dW.append(dW_i)\n",
    "            dB.append(dB_i)\n",
    "        return dW, dB\n",
    "    \n",
    "    def update_WB(self, W, B, learning_rate, dW, dB):\n",
    "        n_layers = len(W)\n",
    "        for i in range(n_layers):\n",
    "            W[i] -= learning_rate*dW[i]\n",
    "            B[i] -= learning_rate*dB[i]\n",
    "        return W, B\n",
    "    \n",
    "    def predict(self, X):\n",
    "        t_X0 = torch(X, torch_type='sample', assigned=True)\n",
    "        t_W = self.to_torch(self.W, torch_type='weight')\n",
    "        t_B = self.to_torch(self.B, torch_type='bias')\n",
    "        t_Z, t_X = self.forward(t_X0, t_W, t_B, self.AF)\n",
    "        torch.clear_adj()\n",
    "        return t_X[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __init__(self, Data, norm_type, min_norm=-1, max_norm=1):\n",
    "        self.norm_type = norm_type\n",
    "        \n",
    "        if self.norm_type == 'minmaxNorm':\n",
    "            self.Data_min = self.min4norm(Data)\n",
    "            self.Data_max = self.max4norm(Data)\n",
    "        elif self.norm_type == 'rescale':\n",
    "            self.Data_min_norm = min_norm\n",
    "            self.Data_max_norm = max_norm\n",
    "            self.Data_min = self.min4norm(Data)\n",
    "            self.Data_max = self.max4norm(Data)\n",
    "        elif self.norm_type == 'meanNorm':\n",
    "            self.Data_min = self.min4norm(Data)\n",
    "            self.Data_max = self.max4norm(Data)\n",
    "            self.Data_mean = self.mean4norm(Data)\n",
    "        elif self.norm_type == 'standardization':\n",
    "            self.Data_mean = self.mean4norm(Data)\n",
    "            self.Data_std = self.std4norm(Data)\n",
    "        elif self.norm_type == 'SUL':\n",
    "            self.Data_ed = self.ed4norm(Data)\n",
    "        \n",
    "    def min4norm(self, Data):\n",
    "        _min = Data.min(axis=0)\n",
    "        return _min.reshape(1, -1)\n",
    "    def max4norm(self, Data):\n",
    "        _max = Data.max(axis=0)\n",
    "        return _max.reshape(1, -1)\n",
    "    def mean4norm(self, Data):\n",
    "        _mean = Data.mean(axis=0)\n",
    "        return _mean.reshape(1, -1)\n",
    "    def std4norm(self, Data):\n",
    "        _std = Data.std(axis=0)\n",
    "        return _std.reshape(1, -1)\n",
    "    def ed4norm(self, Data):\n",
    "        _ed = np.sqrt((Data**2).sum(axis=0))\n",
    "        return _ed.reshape(1, -1)\n",
    "    \n",
    "    def minmaxNorm(self, Data, _min, _max):\n",
    "        if 0 in (_max - _min):\n",
    "            raise Exception('max and min are equal')\n",
    "        Data_norm = (Data - _min)/(_max - _min)\n",
    "        return Data_norm\n",
    "    def rescale(self, Data, _min, _max, min_norm, max_norm):\n",
    "        if max_norm <= min_norm:\n",
    "            raise Exception('max_norm has to greater than min_norm')\n",
    "        if 0 in (_max - _min):\n",
    "            raise Exception('max and min are equal')\n",
    "        Data_norm = (max_norm - min_norm)*(Data - _min)/(_max - _min) + min_norm\n",
    "        return Data_norm\n",
    "    def meanNorm(self, Data, _min, _max, _mean):\n",
    "        if 0 in (_max - _min):\n",
    "            raise Exception('max and min are equal')\n",
    "        Data_norm = (Data - _mean)/(_max - _min)\n",
    "        return Data_norm\n",
    "    def standardization(self, Data, _mean, _std):\n",
    "        if 0 in _std:\n",
    "            raise Exception('std is equal 0')\n",
    "        Data_norm = (Data - _mean)/_std\n",
    "        return Data_norm\n",
    "    def SUL(self, Data, _ed):\n",
    "        if 0 in _ed:\n",
    "            raise Exception('ed is equal 0')\n",
    "        Data_norm = Data/_ed\n",
    "        return Data_norm\n",
    "    \n",
    "    def fit(self, Data):\n",
    "        if self.norm_type == 'minmaxNorm':\n",
    "            Data_norm = self.minmaxNorm(Data, self.Data_min, self.Data_max)\n",
    "        elif self.norm_type == 'rescale':\n",
    "            Data_norm = self.rescale(Data, self.Data_min, self.Data_max, self.Data_min_norm, self.Data_max_norm)\n",
    "        elif self.norm_type == 'meanNorm':\n",
    "            Data_norm = self.meanNorm(Data, self.Data_min, self.Data_max, self.Data_mean)\n",
    "        elif self.norm_type == 'standardization':\n",
    "            Data_norm = self.standardization(Data, self.Data_mean, self.Data_std)\n",
    "        elif self.norm_type == 'SUL':\n",
    "            Data_norm = self.SUL(Data, self.Data_ed)\n",
    "        return Data_norm\n",
    "    \n",
    "    def de_minmaxNorm(self, Data_norm, _min, _max):\n",
    "        Data = Data_norm*(_max - _min) + _min\n",
    "        return Data\n",
    "    def de_rescale(self, Data_norm, _min, _max, min_norm, max_norm):\n",
    "        if max_norm <= min_norm:\n",
    "            raise Exception('max_norm has to greater than min_norm')\n",
    "        Data = (_max - _min)*(Data_norm - min_norm)/(max_norm - min_norm) + _min\n",
    "        return Data\n",
    "    def de_meanNorm(self, Data_norm, _min, _max, _mean):\n",
    "        Data = Data_norm*(_max - _min) + _mean\n",
    "        return Data\n",
    "    def de_standardization(self, Data_norm, _mean, _std):\n",
    "        Data = Data_norm*_std + _mean\n",
    "        return Data\n",
    "    def de_SUL(self, Data_norm, _ed):\n",
    "        Data = Data_norm*_ed\n",
    "        return Data\n",
    "    \n",
    "    def transform(self, Data_norm):\n",
    "        if self.norm_type == 'minmaxNorm':\n",
    "            Data = self.de_minmaxNorm(Data_norm, self.Data_min, self.Data_max)\n",
    "        elif self.norm_type == 'rescale':\n",
    "            Data = self.de_rescale(Data_norm, self.Data_min, self.Data_max, self.Data_min_norm, self.Data_max_norm)\n",
    "        elif self.norm_type == 'meanNorm':\n",
    "            Data = self.de_meanNorm(Data_norm, self.Data_min, self.Data_max, self.Data_mean)\n",
    "        elif self.norm_type == 'standardization':\n",
    "            Data = self.de_standardization(Data_norm, self.Data_mean, self.Data_std)\n",
    "        elif self.norm_type == 'SUL':\n",
    "            Data = self.de_SUL(Data_norm, self.Data_ed)\n",
    "        return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
