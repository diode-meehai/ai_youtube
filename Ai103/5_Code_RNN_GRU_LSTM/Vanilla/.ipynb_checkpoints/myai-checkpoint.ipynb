{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    @classmethod\n",
    "    def unique_value(self, X):\n",
    "        unique_value = np.unique(X)\n",
    "        return unique_value\n",
    "    \n",
    "    @classmethod\n",
    "    def one_hot_matrix(self, X, unique_value):\n",
    "        N = X.shape[0]\n",
    "        n_value = len(unique_value)\n",
    "        one_hot_matrix = np.zeros([N, n_value])\n",
    "        for i, x in enumerate(X[:, 0:1]):\n",
    "            j = np.argwhere(unique_value == x[0]).ravel()[0]\n",
    "            one_hot_matrix[i, j] = 1\n",
    "        return one_hot_matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def create_one_hot_matrix(self, X):\n",
    "        unique_value = self.unique_value(X)\n",
    "        one_hot_matrix = self.one_hot_matrix(X, unique_value)\n",
    "        return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorScore:\n",
    "    @classmethod\n",
    "    def find_error(self, Y, Yhat, error_type):\n",
    "        if error_type == 'SSE':\n",
    "            error = self.find_SSE(Y, Yhat)\n",
    "        elif error_type == 'MSE':\n",
    "            error = self.find_MSE(Y, Yhat)\n",
    "        elif error_type == 'MAE':\n",
    "            error = self.find_MAE(Y, Yhat)\n",
    "        elif error_type == 'MAPE':\n",
    "            error = self.find_MAPE(Y, Yhat)\n",
    "        elif error_type == 'entropy':\n",
    "            error = self.find_entropy(Y, Yhat)\n",
    "        elif error_type == 'binary':\n",
    "            error = self.find_error_bin_class(Y, Yhat)\n",
    "        elif error_type == 'multiclass':\n",
    "            error = self.find_error_mul_class(Y, Yhat)\n",
    "        return error\n",
    "    \n",
    "    @classmethod\n",
    "    def find_SSE(self, Y, Yhat):\n",
    "        SSE = ((Y - Yhat)**2).sum()\n",
    "        return SSE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_MSE(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        SSE = ((Y - Yhat)**2).sum()\n",
    "        MSE = SSE/N\n",
    "        return MSE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_MAE(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        MAE = (np.abs(Y - Yhat)).sum()/N\n",
    "        return MAE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_MAPE(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        MAPE = np.abs((Y - Yhat)/Y).sum()*100/N\n",
    "        return MAPE\n",
    "    \n",
    "    @classmethod\n",
    "    def find_entropy(self, Y, Yhat):\n",
    "        entropy = (-Y*np.log(Yhat)).sum()\n",
    "        return entropy\n",
    "    \n",
    "    @classmethod\n",
    "    def find_error_bin_class(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        _Y = np.round(Y, 0)\n",
    "        _Yhat = np.round(Yhat, 0)\n",
    "        error = 100*(_Y != _Yhat).sum()/N\n",
    "        return error\n",
    "    \n",
    "    @classmethod\n",
    "    def find_error_mul_class(self, Y, Yhat):\n",
    "        N = Y.shape[0]\n",
    "        Y_argmax = np.argmax(Y, axis=1)\n",
    "        Yhat_argmax = np.argmax(Yhat, axis=1)\n",
    "        error = 100*(Y_argmax != Yhat_argmax).sum()/N\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    @classmethod\n",
    "    def r(self, X, Y, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        if X.shape[1] != 1:\n",
    "            raise Exception('n_features must be 1')\n",
    "        if Y.shape[1] != 1:\n",
    "            raise Exception('n_targets must be 1')\n",
    "            \n",
    "        if sample_weight is None:\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "        else:\n",
    "            X = sample_weight*X\n",
    "            Y = sample_weight*Y\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "        fraction = ((X - X_mean)*(Y - Y_mean)).sum()\n",
    "        denominator = np.sqrt(((X- X_mean)**2).sum()*((Y - Y_mean)**2).sum())\n",
    "        r = fraction/denominator\n",
    "        return r\n",
    "            \n",
    "    def fit(self, X, Y, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        if X.shape[1] != 1:\n",
    "            raise Exception('n_features must be 1')\n",
    "        if Y.shape[1] != 1:\n",
    "            raise Exception('n_targets must be 1')\n",
    "            \n",
    "        XY = X*Y\n",
    "        X2 = X**2\n",
    "        if sample_weight is None:\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "            XY_mean = XY.mean()\n",
    "            X2_mean = X2.mean()\n",
    "        else:\n",
    "            X = sample_weight*X\n",
    "            Y = sample_weight*Y\n",
    "            XY = sample_weight*XY\n",
    "            X2 = sample_weight*X2\n",
    "            X_mean = X.mean()\n",
    "            Y_mean = Y.mean()\n",
    "            XY_mean = XY.mean()\n",
    "            X2_mean = X2.mean()\n",
    "        denominator = X2_mean - X_mean**2\n",
    "        self.a = (XY_mean - X_mean*Y_mean)/denominator\n",
    "        self.b = (X2_mean*Y_mean - X_mean*XY_mean)/denominator\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Yhat = self.a*X + self.b\n",
    "        return Yhat\n",
    "    \n",
    "    def scatter(self, X, Y, line=False):\n",
    "        plt.scatter(X, Y)\n",
    "        plt.plot(X, self.a*X + self.b, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleRegression(ErrorScore):\n",
    "    def __init__(self, approach='Global', n_jobs=None):\n",
    "        self.approach = approach\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    def fit(self, X, Y, epoch=1000, learning_rate=0.01, error_type='SSE', sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        if Y.shape[1] != 1:\n",
    "            raise Exception('n_targets must be 1')\n",
    "        N = X.shape[0]\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        if self.approach == 'Global':\n",
    "            front = inv(np.dot(Xb.T, Xb))\n",
    "            back = np.dot(Xb.T, Y)\n",
    "            self.W = np.dot(front, back)\n",
    "        elif self.approach == 'Local':\n",
    "            error_list = []\n",
    "            D = Xb.shape[1]\n",
    "            W = np.random.randn(D, 1)\n",
    "            for i in range(epoch):\n",
    "                Yhat = np.dot(Xb, W)\n",
    "                error = ErrorScore.find_error(Y, Yhat, error_type)\n",
    "                error_list.append(error)\n",
    "                W = W + (learning_rate/N)*np.dot(Xb.T, Y-Yhat)\n",
    "            self.error_list = error_list\n",
    "            self.W = W\n",
    "    \n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        Yhat = np.dot(Xb, self.W)\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    @classmethod\n",
    "    def sigmoid(self, Z):\n",
    "        # Yhat = 1/(1 + np.e**(-Z))\n",
    "        Yhat = 1/(1 + np.exp(-Z)) #faster\n",
    "        return Yhat\n",
    "    \n",
    "    @classmethod\n",
    "    def softmax(self, Z):\n",
    "        Yhat = np.exp(Z)/np.exp(Z).sum(axis=1, keepdims = True)\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(ActivationFunction, ErrorScore):\n",
    "    def __init__(self, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "    \n",
    "    def fit(self, X, Y, epoch=1000, learning_rate=0.01, error_type='entropy', sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        N = X.shape[0]\n",
    "        D = X.shape[1] + 1\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        K = Y.shape[1]\n",
    "        W = np.random.randn(D, K)\n",
    "        error_list = []\n",
    "        for i in range(epoch):\n",
    "            Z = np.dot(Xb, W)\n",
    "            if K == 1:\n",
    "                Yhat = ActivationFunction.sigmoid(Z)\n",
    "            elif K > 1:\n",
    "                Yhat = ActivationFunction.softmax(Z)\n",
    "            \n",
    "            error = ErrorScore.find_error(Y, Yhat, error_type)\n",
    "            error_list.append(error)\n",
    "            W = W + (learning_rate/N)*np.dot(Xb.T, Y-Yhat)\n",
    "        self.error_list = error_list\n",
    "        self.W = W\n",
    "    \n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        Xb = np.hstack([np.ones([N, 1]), X])\n",
    "        Z = np.dot(Xb, self.W)\n",
    "        K = self.W.shape[1]\n",
    "        if K == 1:\n",
    "            Yhat = ActivationFunction.sigmoid(Z)\n",
    "        elif K > 1:\n",
    "            Yhat = ActivationFunction.softmax(Z)\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    adj = []\n",
    "    total_id = 0\n",
    "    e = 2.718281828459045\n",
    "    n_samples = 'wait for assigned'\n",
    "    \n",
    "    def __init__(self, data, represent, torch_type='sample'):\n",
    "        _dict = {}\n",
    "        _dict['id'] = tensor.total_id\n",
    "        _dict['data'] = data\n",
    "        _dict['represent'] = represent\n",
    "        _dict['torch_type'] = torch_type\n",
    "        _dict['created_from'] = 'assigned'\n",
    "        _dict['send_to'] = []\n",
    "        _dict['diff'] = []\n",
    "        _dict['family'] = set()\n",
    "        _dict['storage_chain_rule'] = None\n",
    "        if torch_type == 'sample' and tensor.n_samples == 'wait for assigned':\n",
    "            tensor.n_samples = represent.shape[0]\n",
    "        tensor.adj.append(_dict)\n",
    "        self.id = tensor.total_id\n",
    "        tensor.total_id += 1\n",
    "        self.data = data\n",
    "        \n",
    "    def update_so(self, self_id, other_id, new_var_id, operation):\n",
    "        tensor.adj[self_id]['send_to'].append(new_var_id)\n",
    "        tensor.adj[other_id]['send_to'].append(new_var_id)\n",
    "        tensor.adj[new_var_id]['created_from'] = operation\n",
    "        \n",
    "        self_set = tensor.adj[self_id]['family']\n",
    "        other_set = tensor.adj[other_id]['family']\n",
    "        tensor.adj[new_var_id]['family'].add(self_id)\n",
    "        tensor.adj[new_var_id]['family'].add(other_id)\n",
    "        tensor.adj[new_var_id]['family'] = tensor.adj[new_var_id]['family'] | self_set | other_set\n",
    "        \n",
    "    def update_s(self, self_id, new_var_id, operation):\n",
    "        tensor.adj[self_id]['send_to'].append(new_var_id)\n",
    "        tensor.adj[new_var_id]['created_from'] = operation\n",
    "        \n",
    "        self_set = tensor.adj[self_id]['family']\n",
    "        tensor.adj[new_var_id]['family'].add(self_id)\n",
    "        tensor.adj[new_var_id]['family'] = tensor.adj[new_var_id]['family'] | self_set\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent'] + tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data + other.data, represent)\n",
    "            operation = ('+', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "            tensor.adj[other.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent'] + other\n",
    "            new_var = tensor(self.data + other, represent)\n",
    "            operation = ('+', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other + tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other + self.data, represent)\n",
    "            operation = ('+', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent'] - tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data - other.data, represent)\n",
    "            operation = ('-', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "            tensor.adj[other.id]['diff'].append(-np.ones([tensor.n_samples, 1]))\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent'] - other\n",
    "            new_var = tensor(self.data - other, represent)\n",
    "            operation = ('-', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other - tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other - self.data, represent)\n",
    "            operation = ('-', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(-np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent']*tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data * other.data, represent)\n",
    "            operation = ('*', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[other.id]['diff'].append(tensor.adj[self.id]['represent'])\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent']*other\n",
    "            new_var = tensor(self.data * other, represent)\n",
    "            operation = ('*', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(other * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other*tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other * self.data, represent)\n",
    "            operation = ('*', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(other * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent']/tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data / other.data, represent)\n",
    "            operation = ('/', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(1/tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[other.id]['diff'].append(-tensor.adj[self.id]['represent']/tensor.adj[other.id]['represent']**2)\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent']/other\n",
    "            new_var = tensor(self.data / other, represent)\n",
    "            operation = ('/', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append((1/other) * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other / tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other / self.data, represent)\n",
    "            operation = ('/', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            tensor.adj[self.id]['diff'].append(-other/tensor.adj[self.id]['represent']**2)\n",
    "        return new_var\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = tensor.adj[self.id]['represent'] ** tensor.adj[other.id]['represent']\n",
    "            new_var = tensor(self.data ** other.data, represent)\n",
    "            operation = ('**', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            self_rep = tensor.adj[self.id]['represent']\n",
    "            other_rep = tensor.adj[other.id]['represent']\n",
    "            tensor.adj[self.id]['diff'].append(other_rep*self_rep**(other_rep-1))\n",
    "            tensor.adj[other.id]['diff'].append((self_rep**other_rep)*np.log(self_rep))\n",
    "        else:\n",
    "            represent = tensor.adj[self.id]['represent'] ** other\n",
    "            new_var = tensor(self.data ** other, represent)\n",
    "            operation = ('**', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            self_rep = tensor.adj[self.id]['represent']\n",
    "            tensor.adj[self.id]['diff'].append(other*self_rep**(other-1))\n",
    "        return new_var\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        if not isinstance(other, tensor):\n",
    "            represent = other ** tensor.adj[self.id]['represent']\n",
    "            new_var = tensor(other ** self.data, represent)\n",
    "            operation = ('**', str(other), self.id)\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            self_rep = tensor.adj[self.id]['represent']\n",
    "            tensor.adj[self.id]['diff'].append((other**self_rep)*np.log(other))\n",
    "        return new_var\n",
    "    \n",
    "    def __neg__(self):\n",
    "        represent = -tensor.adj[self.id]['represent']\n",
    "        new_var = tensor(-self.data, represent)\n",
    "        operation = ('-1*', self.id)\n",
    "        self.update_s(self.id, new_var.id, operation)\n",
    "        tensor.adj[self.id]['diff'].append(-np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def __abs__(self):\n",
    "        represent = np.abs(tensor.adj[self.id]['represent'])\n",
    "        new_var = tensor(abs(self.data), represent)\n",
    "        operation = ('abs', self.id)\n",
    "        self.update_s(self.id, new_var.id, operation)\n",
    "        diff_abs = (tensor.adj[self.id]['represent'] > 0) -1*(tensor.adj[self.id]['represent'] < 0)\n",
    "        tensor.adj[self.id]['diff'].append(diff_abs * np.ones([tensor.n_samples, 1]))\n",
    "        return new_var\n",
    "    \n",
    "    def log(self):\n",
    "        represent = np.log(tensor.adj[self.id]['represent'])\n",
    "        new_var = tensor(np.log(self.data), represent)\n",
    "        operation =  ('log', self.id)\n",
    "        self.update_s(self.id, new_var.id, operation)\n",
    "        tensor.adj[self.id]['diff'].append(1/(tensor.adj[self.id]['represent']))\n",
    "        return new_var\n",
    "    \n",
    "    def maximum(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = np.maximum(tensor.adj[self.id]['represent'], tenor.adj[other.id]['represent'])\n",
    "            new_var = tensor(np.maximum(self.data, other.data), represent)\n",
    "            operation = ('maximum', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensnor.adj[self.id]['represent'])\n",
    "            diff_other = (represent == tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "            tensor.adj[other.id]['diff'].append(diff_other)\n",
    "        else:\n",
    "            represent = np.maximum(tensor.adj[self.id]['represent'], other)\n",
    "            new_var = tensor(np.maximum(self.data, other), represent)\n",
    "            operation = ('maximum', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensor.adj[self.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "        return new_var\n",
    "    \n",
    "    def minimum(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            represent = np.minimum(tensor.adj[self.id]['represent'], tenor.adj[other.id]['represent'])\n",
    "            new_var = tensor(np.minimum(self.data, other.data), represent)\n",
    "            operation = ('minimum', self.id, other.id)\n",
    "            self.update_so(self.id, other.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensnor.adj[self.id]['represent'])\n",
    "            diff_other = (represent == tensor.adj[other.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "            tensor.adj[other.id]['diff'].append(diff_other)\n",
    "        else:\n",
    "            represent = np.minimum(tensor.adj[self.id]['represent'], other)\n",
    "            new_var = tensor(np.minimum(self.data, other), represent)\n",
    "            operation = ('minimum', self.id, str(other))\n",
    "            self.update_s(self.id, new_var.id, operation)\n",
    "            diff_self = (represent == tensor.adj[self.id]['represent'])\n",
    "            tensor.adj[self.id]['diff'].append(diff_self)\n",
    "        return new_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch(tensor):\n",
    "    e = 2.718281828459045\n",
    "    n_samples = 'wait for assigned'\n",
    "    \n",
    "    def __init__(self, data, torch_type='sample', assigned=False):\n",
    "        self.data = data\n",
    "        self.torch_type = torch_type\n",
    "        if assigned == True:\n",
    "            if torch_type == 'sample':\n",
    "                self.rep = self.ele_to_tensor(data, torch_type)\n",
    "                if torch.n_samples == 'wait for assigned':\n",
    "                    torch.n_samples = data.shape[0]\n",
    "            elif torch_type == 'weight':\n",
    "                self.rep = self.ele_to_tensor(data, torch_type)\n",
    "            elif torch_type == 'bias':\n",
    "                self.rep = self.ele_to_tensor(data, torch_type)\n",
    "        else:\n",
    "            self.rep = 'wait for assigned'\n",
    "\n",
    "    def ele_to_tensor(self, matrix, torch_type):\n",
    "        if torch_type == 'sample':\n",
    "            sample_matrix = matrix[:1, :]\n",
    "            o_matrix = np.array(sample_matrix, dtype='object')\n",
    "        elif torch_type != 'sample':\n",
    "            o_matrix = np.array(matrix, dtype='object')\n",
    "        n_rows, n_cols = o_matrix.shape\n",
    "        for r in range(n_rows):\n",
    "            for c in range(n_cols):\n",
    "                if torch_type == 'sample':\n",
    "                    o_matrix[r,c] = tensor(o_matrix[r,c], matrix[:,c:c+1], torch_type)\n",
    "                elif torch_type != 'sample':\n",
    "                    o_matrix[r,c] = tensor(o_matrix[r,c], \n",
    "                                    o_matrix[r,c]*np.ones([torch.n_samples, 1]), torch_type)\n",
    "        return o_matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def ele_to_numeric(self, matrix):\n",
    "        matrix = np.array(matrix, dtype='object')\n",
    "        n_rows, n_cols = matrix.shape\n",
    "        for r in range(n_rows):\n",
    "            for c in range(n_cols):\n",
    "                matrix[r,c] = matrix[r,c].data\n",
    "        return matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def clear_adj(self):\n",
    "        tensor.adj = []\n",
    "        tensor.total_id = 0\n",
    "        tensor.n_samples = 'wait for assigned'\n",
    "        torch.n_samples = 'wait for assigned'\n",
    "            \n",
    "    def dot(self, other):\n",
    "        if isinstance(other, tensor):\n",
    "            new_var = torch(np.dot(self.data, other.data))\n",
    "            new_var.rep = np.dot(self.rep, other.rep)\n",
    "        else:\n",
    "            new_var = torch(np.dot(self.data, other))\n",
    "            new_var.rep = np.dot(self.rep, other)\n",
    "        return new_var\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data + other.data)\n",
    "            new_var.rep = self.rep + other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data + other)\n",
    "            new_var.rep = self.rep + other\n",
    "        return new_var\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other + self.data)\n",
    "            new_var.rep = other + self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data - other.data)\n",
    "            new_var.rep = self.rep - other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data - other)\n",
    "            new_var.rep = self.rep - other\n",
    "        return new_var\n",
    "            \n",
    "    def __rsub__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other - self.data)\n",
    "            new_var.rep = other - self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data * other.data)\n",
    "            new_var.rep = self.rep * other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data * other)\n",
    "            new_var.rep = self.rep * other\n",
    "        return new_var\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other * self.data)\n",
    "            new_var.rep = other * self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data / other.data)\n",
    "            new_var.rep = self.rep / other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data / other)\n",
    "            new_var.rep = self.rep / other\n",
    "        return new_var\n",
    "        \n",
    "    def __rtruediv__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other / self.data)\n",
    "            new_var.rep = other / self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, torch):\n",
    "            new_var = torch(self.data ** other.data)\n",
    "            new_var.rep = self.rep ** other.rep\n",
    "        else:\n",
    "            new_var = torch(self.data ** other)\n",
    "            new_var.rep = self.rep ** other\n",
    "        return new_var\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        if not isinstance(other, torch):\n",
    "            new_var = torch(other ** self.data)\n",
    "            new_var.rep = other ** self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def __neg__(self):\n",
    "        new_var = torch(-self.data)\n",
    "        new_var.rep = -self.rep\n",
    "        return new_var\n",
    "    \n",
    "    def abs(self):\n",
    "        new_var = torch(np.abs(self.data))\n",
    "        new_var.rep = np.abs(self.rep)\n",
    "        return new_var\n",
    "    \n",
    "    def log(self):\n",
    "        new_var = torch(np.log(self.data))\n",
    "        new_var.rep = np.log(self.rep)\n",
    "        return new_var\n",
    "\n",
    "    def maximum(self, other):\n",
    "        if isinstance(self, torch):\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.maximum(self.data, other.data))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                max_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        max_tensor = tensor.maximum(self.rep[r,c], other.rep[r,c])\n",
    "                        max_matrix[r,c] = max_tensor\n",
    "                new_var.rep = max_matrix\n",
    "            else:\n",
    "                new_var = torch(np.maximum(self.data, other))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                max_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        max_tensor = tensor.maximum(self.rep[r,c], other)\n",
    "                        max_matrix[r,c] = max_tensor\n",
    "                new_var.rep = max_matrix\n",
    "        else:\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.maximum(self, other.data))\n",
    "                n_rows, n_cols = other.rep.shape\n",
    "                max_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        max_tensor = tensor.maximum(other.rep[r,c], self)\n",
    "                        max_matrix[r,c] = max_tensor\n",
    "                new_var.rep = max_matrix\n",
    "            else:\n",
    "                new_var = torch(np.maximum(self, other))\n",
    "        return new_var\n",
    "    \n",
    "    def minimum(self, other):\n",
    "        if isinstance(self, torch):\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.minimum(self.data, other.data))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                min_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        min_tensor = tensor.minimum(self.rep[r,c], other.rep[r,c])\n",
    "                        min_matrix[r,c] = min_tensor\n",
    "                new_var.rep = min_matrix\n",
    "            else:\n",
    "                new_var = torch(np.minimum(self.data, other))\n",
    "                n_rows, n_cols = self.rep.shape\n",
    "                min_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        min_tensor = tensor.minimum(self.rep[r,c], other)\n",
    "                        min_matrix[r,c] = min_tensor\n",
    "                new_var.rep = min_matrix\n",
    "        else:\n",
    "            if isinstance(other, torch):\n",
    "                new_var = torch(np.minimum(self, other.data))\n",
    "                n_rows, n_cols = other.rep.shape\n",
    "                min_matrix = np.zeros([n_rows, n_cols], dtype='object')\n",
    "                for r in range(n_rows):\n",
    "                    for c in range(n_cols):\n",
    "                        min_tensor = tensor.minimum(other.rep[r,c], self)\n",
    "                        min_matrix[r,c] = min_tensor\n",
    "                new_var.rep = min_matrix\n",
    "            else:\n",
    "                new_var = torch(np.maximum(self, other))\n",
    "        return new_var\n",
    "\n",
    "    def relu(self):\n",
    "        relu = torch.maximum(self, 0)\n",
    "        return relu\n",
    "    \n",
    "    def prelu(self, alpha):\n",
    "        prelu = torch.maximum(self, 0) + alpha*torch.minimum(self, 0)\n",
    "        return prelu\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        sigmoid = 1/(1 + torch.e**(-self))\n",
    "        return sigmoid\n",
    "    \n",
    "    def tanh(self):\n",
    "        tanh = (torch.e**self - torch.e**(-self))/(torch.e**self + torch.e**(-self))\n",
    "        return tanh\n",
    "    \n",
    "    def softmax(self):\n",
    "        eZ = torch.e**self\n",
    "        denominator = torch.col_sum(eZ)\n",
    "        softmax = eZ/denominator\n",
    "        return softmax\n",
    "    \n",
    "    def col_sum(self):\n",
    "        new_var = torch(np.sum(self.data, axis=1, keepdims=True))\n",
    "        new_var.rep = np.sum(self.rep, axis=1, keepdims=True)\n",
    "        return new_var\n",
    "    \n",
    "    def sum(self):\n",
    "        new_var = torch(np.sum(self.data))\n",
    "        new_var.rep = np.sum(self.rep)\n",
    "        return new_var\n",
    "    \n",
    "    def mean(self):\n",
    "        new_var = torch(np.mean(self.data))\n",
    "        # new_var.rep = np.mean(self.rep)\n",
    "        new_var.rep = self.rep/tensor.n_samples\n",
    "        return new_var\n",
    "    \n",
    "    def sse(self, other):\n",
    "        error = (self - other)**2\n",
    "        sse = torch.sum(error)\n",
    "        return sse\n",
    "    \n",
    "    def mse(self, other):\n",
    "        error = (self - other)**2\n",
    "        mse = torch.mean(error)\n",
    "        return mse\n",
    "    \n",
    "    def m2_entropy(self, other):\n",
    "        error = -(self*np.log(other+np.finfo(np.float32).eps) + (1-self)*np.log(1-other+np.finfo(np.float32).eps))\n",
    "        entropy = torch.mean(error)\n",
    "        return entropy\n",
    "    \n",
    "    def m_entropy(self, other):\n",
    "        error = -self*np.log(other+np.finfo(np.float32).eps)\n",
    "        col_error = torch.col_sum(error)\n",
    "        entropy = torch.mean(col_error)\n",
    "        return entropy\n",
    "    \n",
    "    @classmethod\n",
    "    def forward_diff(self, y_id, x_id):\n",
    "        if (x_id in tensor.adj[y_id]['family']) or (x_id == y_id):\n",
    "            if tensor.adj[x_id]['storage_chain_rule'] is not None:\n",
    "                return tensor.adj[x_id]['storage_chain_rule']\n",
    "            else:\n",
    "                send_to = tensor.adj[x_id]['send_to']\n",
    "                n_send = len(send_to)\n",
    "                if n_send == 1:\n",
    "                    target_id = send_to[0]\n",
    "                    dy_dx = tensor.adj[x_id]['diff'][0] * self.forward_diff(y_id, target_id)\n",
    "                    tensor.adj[x_id]['storage_chain_rule'] = dy_dx\n",
    "                    return dy_dx\n",
    "                elif n_send > 1:\n",
    "                    dy_dx = 0\n",
    "                    start_index = n_send - 5\n",
    "                    if start_index <= 0:\n",
    "                        start_index = 0\n",
    "                    for i in range(start_index, n_send):\n",
    "                        target_id = send_to[i]\n",
    "                        dy_dx += tensor.adj[x_id]['diff'][i] * self.forward_diff(y_id, target_id)\n",
    "                    tensor.adj[x_id]['storage_chain_rule'] = dy_dx\n",
    "                    return dy_dx\n",
    "                elif n_send == 0:\n",
    "                    return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def gradient(self):\n",
    "        n_rows, n_cols = self.rep.shape\n",
    "        diff_matrix = np.zeros([n_rows, n_cols])\n",
    "        for r in range(n_rows):\n",
    "            for c in range(n_cols):\n",
    "                diff_matrix[r,c] = np.sum(torch.forward_diff(tensor.adj[-1]['id'], self.rep[r,c].id))\n",
    "        return diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch):\n",
    "    def __init__(self, HL, AF, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "        if len(HL) != len(AF):\n",
    "            raise Exception('n_layers must be same')\n",
    "        self.HL = HL\n",
    "        self.AF = AF\n",
    "        \n",
    "    def fit(self, X, Y, loss_function, epoch=1000, learning_rate=0.01, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_sample must be same')\n",
    "        D = X.shape[1]\n",
    "        W, B = self.create_WB(D, self.HL)\n",
    "        loss_list = []\n",
    "        for i in range(epoch):\n",
    "            t_X0 = torch(X, torch_type='sample', assigned=True)\n",
    "            t_Y = torch(Y, torch_type='sample', assigned=True)\n",
    "            t_W = self.to_torch(W, torch_type='weight')\n",
    "            t_B = self.to_torch(B, torch_type='bias')\n",
    "            t_X = self.forward(t_X0, t_W, t_B, self.AF)\n",
    "            loss = self.compute_loss(t_Y, t_X[-1], loss_function)\n",
    "            loss_list.append(loss.data)\n",
    "            dW, dB = self.compute_gradient(t_W, t_B)\n",
    "            W, B = self.update_WB(W, B, learning_rate, dW, dB)\n",
    "            torch.clear_adj()\n",
    "        self.loss_list = loss_list\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "         \n",
    "    @classmethod\n",
    "    def create_WB(self, D, HL):\n",
    "        W = []\n",
    "        B = []\n",
    "        for i in range(len(HL)):\n",
    "            if i == 0:\n",
    "                W_i = np.random.randn(D, HL[0])/np.sqrt(HL[0])\n",
    "            else:\n",
    "                W_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "            B_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            W.append(W_i)\n",
    "            B.append(B_i)\n",
    "        return W, B\n",
    "    \n",
    "    @classmethod\n",
    "    def to_torch(self, tensor_list, torch_type, assigned=True):\n",
    "        torch_list = []\n",
    "        for i in range(len(tensor_list)):\n",
    "            _torch = torch(tensor_list[i], torch_type, assigned)\n",
    "            torch_list.append(_torch)\n",
    "        return torch_list\n",
    "    \n",
    "    @classmethod\n",
    "    def forward(self, t_X0, t_W, t_B, AF):\n",
    "        n_layers = len(t_W)\n",
    "        Z = []\n",
    "        X = []\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                Z_i = torch.dot(t_X0, t_W[0]) + t_B[0]\n",
    "            else:\n",
    "                Z_i = torch.dot(X_i, t_W[i]) + t_B[i]\n",
    "            X_i = self.compute_X(Z_i, AF[i])\n",
    "            Z.append(Z_i)\n",
    "            X.append(X_i)\n",
    "        return X\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_X(self, Z_i, af):\n",
    "        if isinstance(af, str):\n",
    "            if af == 'sigmoid':\n",
    "                X_i = torch.sigmoid(Z_i)\n",
    "            elif af == 'tanh':\n",
    "                X_i = torch.tanh(Z_i)\n",
    "            elif af == 'relu':\n",
    "                X_i = torch.relu(Z_i)\n",
    "            elif af == 'softmax':\n",
    "                X_i = torch.softmax(Z_i)\n",
    "        elif isinstance(af, list):\n",
    "            if af[0] == 'prelu':\n",
    "                X_i = torch.prelu(Z_i, af[1])\n",
    "        return X_i\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_loss(self, t_Y, t_Yhat, loss_function):\n",
    "        if loss_function == 'sse':\n",
    "            loss = torch.sse(t_Y, t_Yhat)\n",
    "        elif loss_function == 'mse':\n",
    "            loss = torch.mse(t_Y, t_Yhat)\n",
    "        elif loss_function == 'm2_entropy':\n",
    "            loss = torch.m2_entropy(t_Y, t_Yhat)\n",
    "        elif loss_function == 'm_entropy':\n",
    "            loss = torch.m_entropy(t_Y, t_Yhat)\n",
    "        return loss\n",
    "        \n",
    "    @classmethod\n",
    "    def compute_gradient(self, t_W, t_B):\n",
    "        n_layers = len(t_W)\n",
    "        dW = []\n",
    "        dB = []\n",
    "        for i in range(n_layers):\n",
    "            dW_i = torch.gradient(t_W[i])\n",
    "            dB_i = torch.gradient(t_B[i])\n",
    "            dW.append(dW_i)\n",
    "            dB.append(dB_i)\n",
    "        return dW, dB\n",
    "    \n",
    "    @classmethod\n",
    "    def update_WB(self, W, B, learning_rate, dW, dB):\n",
    "        n_layers = len(W)\n",
    "        for i in range(n_layers):\n",
    "            W[i] -= learning_rate*dW[i]\n",
    "            B[i] -= learning_rate*dB[i]\n",
    "        return W, B\n",
    "    \n",
    "    def predict(self, X):\n",
    "        t_X0 = torch(X, torch_type='sample', assigned=True)\n",
    "        t_W = self.to_torch(self.W, torch_type='weight')\n",
    "        t_B = self.to_torch(self.B, torch_type='bias')\n",
    "        t_X = self.forward(t_X0, t_W, t_B, self.AF)\n",
    "        torch.clear_adj()\n",
    "        return t_X[-1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(NN):\n",
    "    def __init__(self, n_ts_feature, n_ts_target, RNN_HL, ANN_HL, ANN_AF, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "        if n_ts_feature < n_ts_target:\n",
    "            raise Exception('timestep of feature must be greater than timestep of target')\n",
    "        if len(ANN_HL) != len(ANN_AF):\n",
    "            raise Exception('n_layers must be same')\n",
    "        self.n_ts_feature = n_ts_feature\n",
    "        self.n_ts_target = n_ts_target\n",
    "        self.RNN_HL = RNN_HL\n",
    "        self.ANN_HL = ANN_HL\n",
    "        self.ANN_AF = ANN_AF\n",
    "    \n",
    "    def fit(self, X, Y, loss_function, epoch=1000, learning_rate=0.01, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        _D = X.shape[1]\n",
    "        _Y = Y.shape[1]\n",
    "        n_features = int(_D/self.n_ts_feature)\n",
    "        n_classes = int(_Y/self.n_ts_target)\n",
    "        X0_TS = self.seperate_timestep(X, self.n_ts_feature)\n",
    "        Y_TS = self.seperate_timestep(Y, self.n_ts_target)\n",
    "        W, U, B = self.create_WUB(n_features, self.RNN_HL)\n",
    "        Wout, Bout = NN.create_WB(self.RNN_HL[-1], self.ANN_HL)\n",
    "        loss_list = []\n",
    "        for i in range(epoch):\n",
    "            t_X0_TS = NN.to_torch(X0_TS, torch_type='sample')\n",
    "            t_Y_TS = NN.to_torch(Y_TS, torch_type='sample')\n",
    "            t_W = NN.to_torch(W, torch_type='weight')\n",
    "            t_U = NN.to_torch(U, torch_type='weight')\n",
    "            t_B = NN.to_torch(B, torch_type='bias')\n",
    "            t_Wout = NN.to_torch(Wout, torch_type='weight')\n",
    "            t_Bout = NN.to_torch(Bout, torch_type='bias')\n",
    "            t_X = self.forward(t_X0_TS, t_W, t_U, t_B, t_Wout, t_Bout)\n",
    "            loss = 0\n",
    "            for j in range(self.n_ts_target):\n",
    "                loss += NN.compute_loss(t_Y_TS[j], t_X[-1][j], loss_function)\n",
    "            loss_list.append(loss.data)\n",
    "            dW, dU, dB, dWout, dBout = self.compute_gradient(t_W, t_U, t_B, t_Wout, t_Bout)\n",
    "            W, U, B, Wout, Bout = self.update_all_weight(W, U, B, Wout, Bout, learning_rate, dW, dU, dB, dWout, dBout)\n",
    "            torch.clear_adj()\n",
    "        self.loss_list = loss_list\n",
    "        self.W = W\n",
    "        self.U = U\n",
    "        self.B = B\n",
    "        self.Wout = Wout\n",
    "        self.Bout = Bout\n",
    "    \n",
    "    @classmethod\n",
    "    def seperate_timestep(self, Data, n_ts):\n",
    "        n_cols = Data.shape[1]\n",
    "        n_features = int(n_cols/n_ts)\n",
    "        Data_TS = []\n",
    "        for i in range(n_ts):\n",
    "            Data_TS_i = Data[:, i*n_features:(i+1)*n_features]\n",
    "            Data_TS.append(Data_TS_i)\n",
    "        return Data_TS\n",
    "    \n",
    "    def create_WUB(self, n_features, HL):\n",
    "        W = []\n",
    "        U = []\n",
    "        B = []\n",
    "        for i in range(len(HL)):\n",
    "            if i == 0:\n",
    "                W_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "            else:\n",
    "                W_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "            U_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            B_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            W.append(W_i)\n",
    "            U.append(U_i)\n",
    "            B.append(B_i)\n",
    "        return W, U, B\n",
    "        \n",
    "    def forward(self, t_X0_TS, t_W, t_U, t_B, t_Wout, t_Bout):\n",
    "        n_RNN_layers = len(t_W)\n",
    "        Z = []\n",
    "        H = []                \n",
    "        for i in range(n_RNN_layers):\n",
    "            Z_i = []\n",
    "            H_i = []\n",
    "            if i == 0: \n",
    "                for j in range(self.n_ts_feature):\n",
    "                    if j == 0:\n",
    "                        Z_ij = torch.dot(t_X0_TS[j], t_W[i]) + t_B[i]\n",
    "                    else:\n",
    "                        Z_ij = torch.dot(t_X0_TS[j], t_W[i]) + torch.dot(H_ij, t_U[i]) + t_B[i]\n",
    "                    H_ij = torch.tanh(Z_ij)\n",
    "                    Z_i.append(Z_ij)\n",
    "                    H_i.append(H_ij)\n",
    "            else:\n",
    "                for j in range(self.n_ts_feature):\n",
    "                    if j == 0:\n",
    "                        Z_ij = torch.dot(H[-1][j], t_W[i]) + t_B[i]\n",
    "                    else:\n",
    "                        Z_ij = torch.dot(H[-1][j], t_W[i]) + torch.dot(H_ij, t_U[i]) + t_B[i]\n",
    "                    H_ij = torch.tanh(Z_ij)\n",
    "                    Z_i.append(Z_ij)\n",
    "                    H_i.append(H_ij)\n",
    "            Z.append(Z_i)\n",
    "            H.append(H_i)\n",
    "        n_ANN_layers = len(t_Wout)\n",
    "        S = []\n",
    "        X = []\n",
    "        for i in range(n_ANN_layers):\n",
    "            S_i = []\n",
    "            X_i = []\n",
    "            if i == 0:\n",
    "                for j in range(self.n_ts_target, 0, -1):\n",
    "                    S_ij = torch.dot(H[-1][-j], t_Wout[i]) + t_Bout[i]\n",
    "                    X_ij = NN.compute_X(S_ij, self.ANN_AF[i])\n",
    "                    S_i.append(S_ij)\n",
    "                    X_i.append(X_ij)\n",
    "            else:\n",
    "                for j in range(self.n_ts_target):\n",
    "                    S_ij = torch.dot(X[-1][j], t_Wout[i]) + t_Bout[i]\n",
    "                    X_ij = NN.compute_X(S_ij, self.ANN_AF[i])\n",
    "                    S_i.append(S_ij)\n",
    "                    X_i.append(X_ij)\n",
    "            S.append(S_i)\n",
    "            X.append(X_i)\n",
    "        return X\n",
    "    \n",
    "    def compute_gradient(self, t_W, t_U, t_B, t_Wout, t_Bout):\n",
    "        n_RNN_layers = len(t_W)\n",
    "        dW = []\n",
    "        dU = []\n",
    "        dB = []\n",
    "        for i in range(n_RNN_layers):\n",
    "            dW_i = torch.gradient(t_W[i])\n",
    "            dU_i = torch.gradient(t_U[i])\n",
    "            dB_i = torch.gradient(t_B[i])\n",
    "            dW.append(dW_i)\n",
    "            dU.append(dU_i)\n",
    "            dB.append(dB_i)\n",
    "        n_ANN_layers = len(t_Wout)\n",
    "        dWout = []\n",
    "        dBout = []\n",
    "        for i in range(n_ANN_layers):\n",
    "            dWout_i = torch.gradient(t_Wout[i])\n",
    "            dBout_i = torch.gradient(t_Bout[i])\n",
    "            dWout.append(dWout_i)\n",
    "            dBout.append(dBout_i)\n",
    "        return dW, dU, dB, dWout, dBout\n",
    "    \n",
    "    def update_all_weight(self, W, U, B, Wout, Bout, learning_rate, dW, dU, dB, dWout, dBout):\n",
    "        n_RNN_layers = len(W)\n",
    "        for i in range(n_RNN_layers):\n",
    "            W[i] -= learning_rate*dW[i]\n",
    "            U[i] -= learning_rate*dU[i]\n",
    "            B[i] -= learning_rate*dB[i]\n",
    "        n_ANN_layers = len(Wout)\n",
    "        for i in range(n_ANN_layers):\n",
    "            Wout[i] -= learning_rate*dWout[i]\n",
    "            Bout[i] -= learning_rate*dBout[i]\n",
    "        return W, U, B, Wout, Bout\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X0_TS = self.seperate_timestep(X, self.n_ts_feature)\n",
    "        t_X0_TS = NN.to_torch(X0_TS, torch_type='sample')\n",
    "        t_W = NN.to_torch(self.W, torch_type='weight')\n",
    "        t_U = NN.to_torch(self.U, torch_type='weight')\n",
    "        t_B = NN.to_torch(self.B, torch_type='bias')\n",
    "        t_Wout = NN.to_torch(self.Wout, torch_type='weight')\n",
    "        t_Bout = NN.to_torch(self.Bout, torch_type='bias')\n",
    "        t_X = self.forward(t_X0_TS, t_W, t_U, t_B, t_Wout, t_Bout)\n",
    "        torch.clear_adj()\n",
    "        return t_X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN, NN):\n",
    "    def __init__(self, n_ts_feature, n_ts_target, RNN_HL, ANN_HL, ANN_AF, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "        if n_ts_feature < n_ts_target:\n",
    "            raise Exception('timestep of feature must be greater than timestep of target')\n",
    "        if len(ANN_HL) != len(ANN_AF):\n",
    "            raise Exception('n_layers must be same')\n",
    "        self.n_ts_feature = n_ts_feature\n",
    "        self.n_ts_target = n_ts_target\n",
    "        self.RNN_HL = RNN_HL\n",
    "        self.ANN_HL = ANN_HL\n",
    "        self.ANN_AF = ANN_AF\n",
    "        \n",
    "    def fit(self, X, Y, loss_function, epoch=1000, learning_rate=0.01, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        _D = X.shape[1]\n",
    "        _Y = Y.shape[1]\n",
    "        n_features = int(_D/self.n_ts_feature)\n",
    "        n_classes = int(_Y/self.n_ts_target)\n",
    "        X0_TS = RNN.seperate_timestep(X, self.n_ts_feature)\n",
    "        Y_TS = RNN.seperate_timestep(Y, self.n_ts_target)\n",
    "        Wr, Wz, Wht, Ur, Uz, Uht, Br, Bz, Bht = self.create_GRU_weight(n_features, self.RNN_HL)\n",
    "        Wout, Bout = NN.create_WB(self.RNN_HL[-1], self.ANN_HL)\n",
    "        loss_list = []\n",
    "        for i in range(epoch):\n",
    "            t_X0_TS = NN.to_torch(X0_TS, torch_type='sample')\n",
    "            t_Y_TS = NN.to_torch(Y_TS, torch_type='sample')\n",
    "            t_Wr = NN.to_torch(Wr, torch_type='weight')\n",
    "            t_Wz = NN.to_torch(Wz, torch_type='weight')\n",
    "            t_Wht = NN.to_torch(Wht, torch_type='weight')\n",
    "            t_Ur = NN.to_torch(Ur, torch_type='weight')\n",
    "            t_Uz = NN.to_torch(Uz, torch_type='weight')\n",
    "            t_Uht = NN.to_torch(Uht, torch_type='weight')\n",
    "            t_Br = NN.to_torch(Br, torch_type='bias')\n",
    "            t_Bz = NN.to_torch(Bz, torch_type='bias')\n",
    "            t_Bht = NN.to_torch(Bht, torch_type='bias')\n",
    "            t_Wout = NN.to_torch(Wout, torch_type='weight')\n",
    "            t_Bout = NN.to_torch(Bout, torch_type='bias')\n",
    "            t_X = self.forward(t_X0_TS, t_Wr, t_Wz, t_Wht, t_Ur, t_Uz, t_Uht, \n",
    "                               t_Br, t_Bz, t_Bht, t_Wout, t_Bout)\n",
    "            loss = 0\n",
    "            for j in range(self.n_ts_target):\n",
    "                loss += NN.compute_loss(t_Y_TS[j], t_X[-1][j], loss_function)\n",
    "            loss_list.append(loss.data)\n",
    "            dWr, dWz, dWht, dUr, dUz, dUht, dBr, dBz, dBht, dWout, dBout = self.compute_gradient(t_Wr, t_Wz, t_Wht, t_Ur, t_Uz, t_Uht, \n",
    "                                                                                                 t_Br, t_Bz, t_Bht, t_Wout, t_Bout)\n",
    "            Wr, Wz, Wht, Ur, Uz, Uht, Br, Bz, Bht, Wout, Bout = self.update_all_weight(Wr, Wz, Wht, Ur, Uz, Uht, \n",
    "                                                                                       Br, Bz, Bht, Wout, Bout, \n",
    "                                                                                       learning_rate, dWr, dWz, dWht, \n",
    "                                                                                       dUr, dUz, dUht, dBr, dBz, dBht, dWout, dBout)\n",
    "            torch.clear_adj()\n",
    "        self.loss_list = loss_list\n",
    "        self.Wr = Wr; self.Wz = Wz; self.Wht = Wht\n",
    "        self.Ur = Ur; self.Uz = Uz; self.Uht = Uht\n",
    "        self.Br = Br; self.Bz = Bz; self.Bht = Bht\n",
    "        self.Wout = Wout; self.Bout = Bout\n",
    "        \n",
    "    def create_GRU_weight(self, n_features, HL):\n",
    "        Wr = []; Wz = []; Wht = []\n",
    "        Ur = []; Uz = []; Uht = []\n",
    "        Br = []; Bz = []; Bht = []\n",
    "        for i in range(len(HL)):\n",
    "            if i == 0:\n",
    "                Wr_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "                Wz_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "                Wht_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "            else:\n",
    "                Wr_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "                Wz_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "                Wht_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "            Ur_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            Uz_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            Uht_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            Br_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            Bz_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            Bht_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            \n",
    "            Wr.append(Wr_i); Wz.append(Wz_i); Wht.append(Wht_i)\n",
    "            Ur.append(Ur_i); Uz.append(Uz_i); Uht.append(Uht_i)\n",
    "            Br.append(Br_i); Bz.append(Bz_i); Bht.append(Bht_i)\n",
    "        return Wr, Wz, Wht, Ur, Uz, Uht, Br, Bz, Bht\n",
    "        \n",
    "    def forward(self, t_X0_TS, t_Wr, t_Wz, t_Wht, t_Ur, t_Uz, t_Uht, t_Br, t_Bz, t_Bht, t_Wout, t_Bout):\n",
    "        n_RNN_layers = len(t_Wr)\n",
    "        R = []\n",
    "        Z = []\n",
    "        HT = []\n",
    "        H = []\n",
    "        for i in range(n_RNN_layers):\n",
    "            R_i = []\n",
    "            Z_i = []\n",
    "            HT_i = []\n",
    "            H_i = []\n",
    "            if i == 0:\n",
    "                for j in range(self.n_ts_feature):\n",
    "                    if j == 0:\n",
    "                        R_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wr[i]) + t_Br[i])\n",
    "                        Z_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wz[i]) + t_Bz[i])\n",
    "                        HT_ij = torch.tanh(torch.dot(t_X0_TS[j], t_Wht[i]) + t_Bht[i])\n",
    "                        H_ij = Z_ij*HT_ij\n",
    "                    else:\n",
    "                        R_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wr[i]) + torch.dot(H_ij, t_Ur[i]) + t_Br[i])\n",
    "                        Z_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wz[i]) + torch.dot(H_ij, t_Uz[i]) + t_Bz[i])\n",
    "                        HT_ij = torch.tanh(torch.dot(t_X0_TS[j], t_Wht[i]) + torch.dot(H_ij*R_ij, t_Uht[i]) + t_Bht[i])\n",
    "                        H_ij = H_ij*(1-Z_ij) + Z_ij*HT_ij\n",
    "                    R_i.append(R_ij)\n",
    "                    Z_i.append(Z_ij)\n",
    "                    HT_i.append(HT_ij)\n",
    "                    H_i.append(H_ij)\n",
    "            else:\n",
    "                for j in range(self.n_ts_feature):\n",
    "                    if j == 0:\n",
    "                        R_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wr[i]) + t_Br[i])\n",
    "                        Z_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wz[i]) + t_Bz[i])\n",
    "                        HT_ij = torch.tanh(torch.dot(H[-1][j], t_Wht[i]) + t_Bht[i])\n",
    "                        H_ij = Z_ij*HT_ij\n",
    "                    else:\n",
    "                        R_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wr[i]) + torch.dot(H_ij, t_Ur[i]) + t_Br[i])\n",
    "                        Z_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wz[i]) + torch.dot(H_ij, t_Uz[i]) + t_Bz[i])\n",
    "                        HT_ij = torch.tanh(torch.dot(H[-1][j], t_Wht[i]) + torch.dot(H_ij*R_ij, t_Uht[i]) + t_Bht[i])\n",
    "                        H_ij = H_ij*(1-Z_ij) + Z_ij*HT_ij\n",
    "                    R_i.append(R_ij)\n",
    "                    Z_i.append(Z_ij)\n",
    "                    HT_i.append(HT_ij)\n",
    "                    H_i.append(H_ij)\n",
    "            R.append(R_i)\n",
    "            Z.append(Z_i)\n",
    "            HT.append(HT_i)\n",
    "            H.append(H_i)\n",
    "        n_ANN_layers = len(t_Wout)\n",
    "        S = []\n",
    "        X = []\n",
    "        for i in range(n_ANN_layers):\n",
    "            S_i = []\n",
    "            X_i = []\n",
    "            if i == 0:\n",
    "                for j in range(self.n_ts_target, 0, -1):\n",
    "                    S_ij = torch.dot(H[-1][-j], t_Wout[i]) + t_Bout[i]\n",
    "                    X_ij = NN.compute_X(S_ij, self.ANN_AF[i])\n",
    "                    S_i.append(S_ij)\n",
    "                    X_i.append(X_ij)\n",
    "            else:\n",
    "                for j in range(self.n_ts_target):\n",
    "                    S_ij = torch.dot(X[-1][j], t_Wout[i]) + t_Bout[i]\n",
    "                    X_ij = NN.compute_X(S_ij, self.ANN_AF[i])\n",
    "                    S_i.append(S_ij)\n",
    "                    X_i.append(X_ij)\n",
    "            S.append(S_i)\n",
    "            X.append(X_i)\n",
    "        return X\n",
    "    \n",
    "    def compute_gradient(self, t_Wr, t_Wz, t_Wht, t_Ur, t_Uz, t_Uht, t_Br, t_Bz, t_Bht, t_Wout, t_Bout):\n",
    "        n_RNN_layers = len(t_Wr)\n",
    "        dWr = []; dWz = []; dWht = []\n",
    "        dUr = []; dUz = []; dUht = []\n",
    "        dBr = []; dBz = []; dBht = []\n",
    "        for i in range(n_RNN_layers):\n",
    "            dWr_i = torch.gradient(t_Wr[i])\n",
    "            dWz_i = torch.gradient(t_Wz[i])\n",
    "            dWht_i = torch.gradient(t_Wht[i])\n",
    "            dUr_i = torch.gradient(t_Ur[i])\n",
    "            dUz_i = torch.gradient(t_Uz[i])\n",
    "            dUht_i = torch.gradient(t_Uht[i])\n",
    "            dBr_i = torch.gradient(t_Br[i])\n",
    "            dBz_i = torch.gradient(t_Bz[i])\n",
    "            dBht_i = torch.gradient(t_Bht[i])\n",
    "            \n",
    "            dWr.append(dWr_i); dWz.append(dWz_i); dWht.append(dWht_i)\n",
    "            dUr.append(dUr_i); dUz.append(dUz_i); dUht.append(dUht_i)\n",
    "            dBr.append(dBr_i); dBz.append(dBz_i); dBht.append(dBht_i)\n",
    "        n_ANN_layers = len(t_Wout)\n",
    "        dWout = []\n",
    "        dBout = []\n",
    "        for i in range(n_ANN_layers):\n",
    "            dWout_i = torch.gradient(t_Wout[i])\n",
    "            dBout_i = torch.gradient(t_Bout[i])\n",
    "            dWout.append(dWout_i)\n",
    "            dBout.append(dBout_i)\n",
    "        return dWr, dWz, dWht, dUr, dUz, dUht, dBr, dBz, dBht, dWout, dBout\n",
    "    \n",
    "    def update_all_weight(self, Wr, Wz, Wht, Ur, Uz, Uht, Br, Bz, Bht, Wout, Bout, learning_rate, dWr, dWz, dWht, dUr, dUz, dUht, dBr, dBz, dBht, dWout, dBout):\n",
    "        n_RNN_layers = len(Wr)\n",
    "        for i in range(n_RNN_layers):\n",
    "            Wr[i] -= learning_rate*dWr[i]\n",
    "            Wz[i] -= learning_rate*dWz[i]\n",
    "            Wht[i] -= learning_rate*dWht[i]\n",
    "            Ur[i] -= learning_rate*dUr[i]\n",
    "            Uz[i] -= learning_rate*dUz[i]\n",
    "            Uht[i] -= learning_rate*dUht[i]\n",
    "            Br[i] -= learning_rate*dBr[i]\n",
    "            Bz[i] -= learning_rate*dBz[i]\n",
    "            Bht[i] -= learning_rate*dBht[i]\n",
    "        n_ANN_layers = len(Wout)\n",
    "        for i in range(n_ANN_layers):\n",
    "            Wout[i] -= learning_rate*dWout[i]\n",
    "            Bout[i] -= learning_rate*dBout[i]\n",
    "        return Wr, Wz, Wht, Ur, Uz, Uht, Br, Bz, Bht, Wout, Bout\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X0_TS = RNN.seperate_timestep(X, self.n_ts_feature)\n",
    "        t_X0_TS = NN.to_torch(X0_TS, torch_type='sample')\n",
    "        t_Wr = NN.to_torch(self.Wr, torch_type='weight')\n",
    "        t_Wz = NN.to_torch(self.Wz, torch_type='weight')\n",
    "        t_Wht = NN.to_torch(self.Wht, torch_type='weight')\n",
    "        t_Ur = NN.to_torch(self.Ur, torch_type='weight')\n",
    "        t_Uz = NN.to_torch(self.Uz, torch_type='weight')\n",
    "        t_Uht = NN.to_torch(self.Uht, torch_type='weight')\n",
    "        t_Br = NN.to_torch(self.Br, torch_type='bias')\n",
    "        t_Bz = NN.to_torch(self.Bz, torch_type='bias')\n",
    "        t_Bht = NN.to_torch(self.Bht, torch_type='bias')\n",
    "        t_Wout = NN.to_torch(self.Wout, torch_type='weight')\n",
    "        t_Bout = NN.to_torch(self.Bout, torch_type='bias')\n",
    "        t_X = self.forward(t_X0_TS, t_Wr, t_Wz, t_Wht, t_Ur, t_Uz, t_Uht, \n",
    "                           t_Br, t_Bz, t_Bht, t_Wout, t_Bout)\n",
    "        torch.clear_adj()\n",
    "        return t_X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(RNN, NN):\n",
    "    def __init__(self, n_ts_feature, n_ts_target, RNN_HL, ANN_HL, ANN_AF, n_jobs=None):\n",
    "        self.n_jobs = n_jobs\n",
    "        if n_ts_feature < n_ts_target:\n",
    "            raise Exception('timestep of feature must be greater than timestep of target')\n",
    "        if len(ANN_HL) != len(ANN_AF):\n",
    "            raise Exception('n_layers must be same')\n",
    "        self.n_ts_feature = n_ts_feature\n",
    "        self.n_ts_target = n_ts_target\n",
    "        self.RNN_HL = RNN_HL\n",
    "        self.ANN_HL = ANN_HL\n",
    "        self.ANN_AF = ANN_AF\n",
    "        \n",
    "    def fit(self, X, Y, loss_function, epoch=1000, learning_rate=0.01, sample_weight=None):\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise Exception('n_samples must be same')\n",
    "        _D = X.shape[1]\n",
    "        _Y = Y.shape[1]\n",
    "        n_features = int(_D/self.n_ts_feature)\n",
    "        n_classes = int(_Y/self.n_ts_target)\n",
    "        X0_TS = RNN.seperate_timestep(X, self.n_ts_feature)\n",
    "        Y_TS = RNN.seperate_timestep(Y, self.n_ts_target)\n",
    "        Wf, Wi, Wct, Wo, Uf, Ui, Uct, Uo, Bf, Bi, Bct, Bo = self.create_LSTM_weight(n_features, self.RNN_HL)\n",
    "        Wout, Bout = NN.create_WB(self.RNN_HL[-1], self.ANN_HL)\n",
    "        loss_list = []\n",
    "        for i in range(epoch):\n",
    "            t_X0_TS = NN.to_torch(X0_TS, torch_type='sample')\n",
    "            t_Y_TS = NN.to_torch(Y_TS, torch_type='sample')\n",
    "            t_Wf = NN.to_torch(Wf, torch_type='weight')\n",
    "            t_Wi = NN.to_torch(Wi, torch_type='weight')\n",
    "            t_Wct = NN.to_torch(Wct, torch_type='weight')\n",
    "            t_Wo = NN.to_torch(Wo, torch_type='weight')\n",
    "            t_Uf = NN.to_torch(Uf, torch_type='weight')\n",
    "            t_Ui = NN.to_torch(Ui, torch_type='weight')\n",
    "            t_Uct = NN.to_torch(Uct, torch_type='weight')\n",
    "            t_Uo = NN.to_torch(Uo, torch_type='weight')\n",
    "            t_Bf = NN.to_torch(Bf, torch_type='bias')\n",
    "            t_Bi = NN.to_torch(Bi, torch_type='bias')\n",
    "            t_Bct = NN.to_torch(Bct, torch_type='bias')\n",
    "            t_Bo = NN.to_torch(Bo, torch_type='bias')\n",
    "            t_Wout = NN.to_torch(Wout, torch_type='weight')\n",
    "            t_Bout = NN.to_torch(Bout, torch_type='bias')\n",
    "            t_X = self.forward(t_X0_TS, t_Wf, t_Wi, t_Wct, t_Wo, \n",
    "                              t_Uf, t_Ui, t_Uct, t_Uo, \n",
    "                              t_Bf, t_Bi, t_Bct, t_Bo, t_Wout, t_Bout)\n",
    "            loss = 0\n",
    "            for j in range(self.n_ts_target):\n",
    "                loss += NN.compute_loss(t_Y_TS[j], t_X[-1][j], loss_function)\n",
    "            loss_list.append(loss.data)\n",
    "            dWf, dWi, dWct, dWo, dUf, dUi, dUct, dUo, dBf, dBi, dBct, dBo, dWout, dBout = self.compute_gradient(t_Wf, t_Wi, t_Wct, t_Wo, \n",
    "                                                                                                 t_Uf, t_Ui, t_Uct, t_Uo, \n",
    "                                                                                                 t_Bf, t_Bi, t_Bct, t_Bo, t_Wout, t_Bout)\n",
    "            Wf, Wi, Wct, Wo, Uf, Ui, Uct, Uo, Bf, Bi, Bct, Bo, Wout, Bout = self.update_all_weight(Wf, Wi, Wct, Wo, \n",
    "                                                                                                   Uf, Ui, Uct, Uo, \n",
    "                                                                                                   Bf, Bi, Bct, Bo, Wout, Bout, \n",
    "                                                                                                   learning_rate, dWf, dWi, dWct, dWo, \n",
    "                                                                                                   dUf, dUi, dUct, dUo, \n",
    "                                                                                                   dBf, dBi, dBct, dBo, dWout, dBout)\n",
    "            torch.clear_adj()\n",
    "        self.loss_list = loss_list\n",
    "        self.Wf = Wf; self.Wi = Wi\n",
    "        self.Wct = Wct; self.Wo = Wo\n",
    "        self.Uf = Uf; self.Ui = Ui\n",
    "        self.Uct = Uct; self.Uo = Uo\n",
    "        self.Bf = Bf; self.Bi = Bi\n",
    "        self.Bct = Bct; self.Bo = Bo\n",
    "        self.Wout = Wout; self.Bout = Bout\n",
    "        \n",
    "    def create_LSTM_weight(self, n_features, HL):\n",
    "        Wf = []; Wi = []; Wct = []; Wo = []\n",
    "        Uf = []; Ui = []; Uct = []; Uo = []\n",
    "        Bf = []; Bi = []; Bct = []; Bo = []\n",
    "        for i in range(len(HL)):\n",
    "            if i == 0:\n",
    "                Wf_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "                Wi_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "                Wct_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "                Wo_i = np.random.randn(n_features, HL[0])/np.sqrt(HL[0])\n",
    "            else:\n",
    "                Wf_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "                Wi_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "                Wct_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "                Wo_i = np.random.randn(HL[i-1], HL[i])/np.sqrt(HL[i])\n",
    "            Uf_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            Ui_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            Uct_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            Uo_i = np.random.randn(HL[i], HL[i])/np.sqrt(HL[i])\n",
    "            Bf_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            Bi_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            Bct_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            Bo_i = np.random.randn(1, HL[i])/np.sqrt(HL[i])\n",
    "            \n",
    "            Wf.append(Wf_i); Wi.append(Wi_i)\n",
    "            Wct.append(Wct_i); Wo.append(Wo_i)\n",
    "            Uf.append(Uf_i); Ui.append(Ui_i)\n",
    "            Uct.append(Uct_i); Uo.append(Uo_i)\n",
    "            Bf.append(Bf_i); Bi.append(Bi_i)\n",
    "            Bct.append(Bct_i); Bo.append(Bo_i)\n",
    "        return Wf, Wi, Wct, Wo, Uf, Ui, Uct, Uo, Bf, Bi, Bct, Bo \n",
    "        \n",
    "    def forward(self, t_X0_TS, t_Wf, t_Wi, t_Wct, t_Wo, t_Uf, t_Ui, t_Uct, t_Uo, t_Bf, t_Bi, t_Bct, t_Bo, t_Wout, t_Bout):\n",
    "        n_RNN_layers = len(t_Wf)\n",
    "        F = []\n",
    "        I = []\n",
    "        CT = []\n",
    "        O = []\n",
    "        C = []\n",
    "        H = []\n",
    "        for i in range(n_RNN_layers):\n",
    "            F_i = []\n",
    "            I_i = []\n",
    "            CT_i = []\n",
    "            O_i = []\n",
    "            C_i = []\n",
    "            H_i = []\n",
    "            if i == 0:\n",
    "                for j in range(self.n_ts_feature):\n",
    "                    if j == 0:\n",
    "                        F_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wf[i]) + t_Bf[i])\n",
    "                        I_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wi[i]) + t_Bi[i])\n",
    "                        CT_ij = torch.tanh(torch.dot(t_X0_TS[j], t_Wct[i]) + t_Bct[i])\n",
    "                        O_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wo[i]) + t_Bo[i])\n",
    "                        C_ij = I_ij*CT_ij\n",
    "                        H_ij = torch.tanh(O_ij*C_ij)\n",
    "                    else:\n",
    "                        F_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wf[i]) + torch.dot(H_ij, t_Uf[i])  + t_Bf[i])\n",
    "                        I_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wi[i]) + torch.dot(H_ij, t_Ui[i]) + t_Bi[i]) \n",
    "                        CT_ij = torch.tanh(torch.dot(t_X0_TS[j], t_Wct[i]) + torch.dot(H_ij, t_Uct[i]) + t_Bct[i])\n",
    "                        O_ij = torch.sigmoid(torch.dot(t_X0_TS[j], t_Wo[i]) + torch.dot(H_ij, t_Uo[i]) + t_Bo[i])\n",
    "                        C_ij = C_ij*F_ij + I_ij*CT_ij\n",
    "                        H_ij = torch.tanh(O_ij*C_ij)\n",
    "                    F_i.append(F_ij)\n",
    "                    I_i.append(I_ij)\n",
    "                    CT_i.append(CT_ij)\n",
    "                    O_i.append(O_ij)\n",
    "                    C_i.append(C_ij)\n",
    "                    H_i.append(H_ij)\n",
    "            else:\n",
    "                for j in range(self.n_ts_feature):\n",
    "                    if j == 0:\n",
    "                        F_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wf[i]) + t_Bf[i])\n",
    "                        I_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wi[i]) + t_Bi[i])\n",
    "                        CT_ij = torch.tanh(torch.dot(H[-1][j], t_Wct[i]) + t_Bct[i])\n",
    "                        O_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wo[i]) + t_Bo[i])\n",
    "                        C_ij = I_ij*CT_ij\n",
    "                        H_ij = torch.tanh(O_ij*C_ij)\n",
    "                    else:\n",
    "                        F_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wf[i]) + torch.dot(H_ij, t_Uf[i]) + t_Bf[i])\n",
    "                        I_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wi[i]) + torch.dot(H_ij, t_Ui[i]) + t_Bi[i])\n",
    "                        CT_ij = torch.tanh(torch.dot(H[-1][j], t_Wct[i]) + torch.dot(H_ij, t_Uct[i]) + t_Bct[i])\n",
    "                        O_ij = torch.sigmoid(torch.dot(H[-1][j], t_Wo[i]) + torch.dot(H_ij, t_Uo[i]) + t_Bo[i])\n",
    "                        C_ij = C_ij*F_ij + I_ij*CT_ij\n",
    "                        H_ij = torch.tanh(O_ij*C_ij)\n",
    "                    F_i.append(F_ij)\n",
    "                    I_i.append(I_ij)\n",
    "                    CT_i.append(CT_ij)\n",
    "                    O_i.append(O_ij)\n",
    "                    C_i.append(C_ij)\n",
    "                    H_i.append(H_ij)\n",
    "            F.append(F_i)\n",
    "            I.append(I_i)\n",
    "            CT.append(CT_i)\n",
    "            O.append(O_i)\n",
    "            C.append(C_i)\n",
    "            H.append(H_i)\n",
    "        n_ANN_layers = len(t_Wout)\n",
    "        S = []\n",
    "        X = []\n",
    "        for i in range(n_ANN_layers):\n",
    "            S_i = []\n",
    "            X_i = []\n",
    "            if i == 0:\n",
    "                for j in range(self.n_ts_target, 0, -1):\n",
    "                    S_ij = torch.dot(H[-1][-j], t_Wout[i]) + t_Bout[i]\n",
    "                    X_ij = NN.compute_X(S_ij, self.ANN_AF[i])\n",
    "                    S_i.append(S_ij)\n",
    "                    X_i.append(X_ij)\n",
    "            else:\n",
    "                for j in range(self.n_ts_target):\n",
    "                    S_ij = torch.dot(X[-1][j], t_Wout[i]) + t_Bout[i]\n",
    "                    X_ij = NN.compute_X(S_ij, self.ANN_AF[i])\n",
    "                    S_i.append(S_ij)\n",
    "                    X_i.append(X_ij)\n",
    "            S.append(S_i)\n",
    "            X.append(X_i)\n",
    "        return X\n",
    "    \n",
    "    def compute_gradient(self, t_Wf, t_Wi, t_Wct, t_Wo, t_Uf, t_Ui, t_Uct, t_Uo, t_Bf, t_Bi, t_Bct, t_Bo, t_Wout, t_Bout):\n",
    "        n_RNN_layers = len(t_Wf)\n",
    "        dWf = []; dWi = []; dWct = []; dWo = []\n",
    "        dUf = []; dUi = []; dUct = []; dUo = []\n",
    "        dBf = []; dBi = []; dBct = []; dBo = []\n",
    "        for i in range(n_RNN_layers):\n",
    "            dWf_i = torch.gradient(t_Wf[i])\n",
    "            dWi_i = torch.gradient(t_Wi[i])\n",
    "            dWct_i = torch.gradient(t_Wct[i])\n",
    "            dWo_i = torch.gradient(t_Wo[i])\n",
    "            dUf_i = torch.gradient(t_Uf[i])\n",
    "            dUi_i = torch.gradient(t_Ui[i])\n",
    "            dUct_i = torch.gradient(t_Uct[i])\n",
    "            dUo_i = torch.gradient(t_Uo[i])\n",
    "            dBf_i = torch.gradient(t_Bf[i])\n",
    "            dBi_i = torch.gradient(t_Bi[i])\n",
    "            dBct_i = torch.gradient(t_Bct[i])\n",
    "            dBo_i = torch.gradient(t_Bo[i])\n",
    "            \n",
    "            dWf.append(dWf_i); dWi.append(dWi_i)\n",
    "            dWct.append(dWct_i); dWo.append(dWo_i)\n",
    "            dUf.append(dUf_i); dUi.append(dUi_i)\n",
    "            dUct.append(dUct_i); dUo.append(dUo_i)\n",
    "            dBf.append(dBf_i); dBi.append(dBi_i)\n",
    "            dBct.append(dBct_i); dBo.append(dBo_i)\n",
    "        n_ANN_layers = len(t_Wout)\n",
    "        dWout = []\n",
    "        dBout = []\n",
    "        for i in range(n_ANN_layers):\n",
    "            dWout_i = torch.gradient(t_Wout[i])\n",
    "            dBout_i = torch.gradient(t_Bout[i])\n",
    "            dWout.append(dWout_i)\n",
    "            dBout.append(dBout_i)\n",
    "        return dWf, dWi, dWct, dWo, dUf, dUi, dUct, dUo, dBf, dBi, dBct, dBo, dWout, dBout\n",
    "    \n",
    "    def update_all_weight(self, Wf, Wi, Wct, Wo, Uf, Ui, Uct, Uo, Bf, Bi, Bct, Bo, Wout, Bout, learning_rate, dWf, dWi, dWct, dWo, dUf, dUi, dUct, dUo, dBf, dBi, dBct, dBo, dWout, dBout):\n",
    "        n_RNN_layers = len(Wf)\n",
    "        for i in range(n_RNN_layers):\n",
    "            Wf[i] -= learning_rate*dWf[i]\n",
    "            Wi[i] -= learning_rate*dWi[i]\n",
    "            Wct[i] -= learning_rate*dWct[i]\n",
    "            Wo[i] -= learning_rate*dWo[i]\n",
    "            Uf[i] -= learning_rate*dUf[i]\n",
    "            Ui[i] -= learning_rate*dUi[i]\n",
    "            Uct[i] -= learning_rate*dUct[i]\n",
    "            Uo[i] -= learning_rate*dUo[i]\n",
    "            Bf[i] -= learning_rate*dBf[i]\n",
    "            Bi[i] -= learning_rate*dBi[i]\n",
    "            Bct[i] -= learning_rate*dBct[i]\n",
    "            Bo[i] -= learning_rate*dBo[i]\n",
    "        n_ANN_layers = len(Wout)\n",
    "        for i in range(n_ANN_layers):\n",
    "            Wout[i] -= learning_rate*dWout[i]\n",
    "            Bout[i] -= learning_rate*dBout[i]\n",
    "        return Wf, Wi, Wct, Wo, Uf, Ui, Uct, Uo, Bf, Bi, Bct, Bo, Wout, Bout\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X0_TS = RNN.seperate_timestep(X, self.n_ts_feature)\n",
    "        t_X0_TS = NN.to_torch(X0_TS, torch_type='sample')\n",
    "        t_Wf = NN.to_torch(self.Wf, torch_type='weight')\n",
    "        t_Wi = NN.to_torch(self.Wi, torch_type='weight')\n",
    "        t_Wct = NN.to_torch(self.Wct, torch_type='weight')\n",
    "        t_Wo = NN.to_torch(self.Wo, torch_type='weight')\n",
    "        t_Uf = NN.to_torch(self.Uf, torch_type='weight')\n",
    "        t_Ui = NN.to_torch(self.Ui, torch_type='weight')\n",
    "        t_Uct = NN.to_torch(self.Uct, torch_type='weight')\n",
    "        t_Uo = NN.to_torch(self.Uo, torch_type='weight')\n",
    "        t_Bf = NN.to_torch(self.Bf, torch_type='bias')\n",
    "        t_Bi = NN.to_torch(self.Bi, torch_type='bias')\n",
    "        t_Bct = NN.to_torch(self.Bct, torch_type='bias')\n",
    "        t_Bo = NN.to_torch(self.Bo, torch_type='bias')\n",
    "        t_Wout = NN.to_torch(self.Wout, torch_type='weight')\n",
    "        t_Bout = NN.to_torch(self.Bout, torch_type='bias')\n",
    "        t_X = self.forward(t_X0_TS, t_Wf, t_Wi, t_Wct, t_Wo, \n",
    "                          t_Uf, t_Ui, t_Uct, t_Uo, \n",
    "                          t_Bf, t_Bi, t_Bct, t_Bo, t_Wout, t_Bout)\n",
    "        torch.clear_adj()\n",
    "        return t_X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __init__(self, Data, norm_type, min_norm=-1, max_norm=1):\n",
    "        self.norm_type = norm_type\n",
    "        \n",
    "        if self.norm_type == 'minmaxNorm':\n",
    "            self.Data_min = self.min4norm(Data)\n",
    "            self.Data_max = self.max4norm(Data)\n",
    "        elif self.norm_type == 'rescale':\n",
    "            self.Data_min_norm = min_norm\n",
    "            self.Data_max_norm = max_norm\n",
    "            self.Data_min = self.min4norm(Data)\n",
    "            self.Data_max = self.max4norm(Data)\n",
    "        elif self.norm_type == 'meanNorm':\n",
    "            self.Data_min = self.min4norm(Data)\n",
    "            self.Data_max = self.max4norm(Data)\n",
    "            self.Data_mean = self.mean4norm(Data)\n",
    "        elif self.norm_type == 'standardization':\n",
    "            self.Data_mean = self.mean4norm(Data)\n",
    "            self.Data_std = self.std4norm(Data)\n",
    "        elif self.norm_type == 'SUL':\n",
    "            self.Data_ed = self.ed4norm(Data)\n",
    "        \n",
    "    def min4norm(self, Data):\n",
    "        _min = Data.min(axis=0)\n",
    "        return _min.reshape(1, -1)\n",
    "    def max4norm(self, Data):\n",
    "        _max = Data.max(axis=0)\n",
    "        return _max.reshape(1, -1)\n",
    "    def mean4norm(self, Data):\n",
    "        _mean = Data.mean(axis=0)\n",
    "        return _mean.reshape(1, -1)\n",
    "    def std4norm(self, Data):\n",
    "        _std = Data.std(axis=0)\n",
    "        return _std.reshape(1, -1)\n",
    "    def ed4norm(self, Data):\n",
    "        _ed = np.sqrt((Data**2).sum(axis=0))\n",
    "        return _ed.reshape(1, -1)\n",
    "    \n",
    "    def minmaxNorm(self, Data, _min, _max):\n",
    "        if 0 in (_max - _min):\n",
    "            raise Exception('max and min are equal')\n",
    "        Data_norm = (Data - _min)/(_max - _min)\n",
    "        return Data_norm\n",
    "    def rescale(self, Data, _min, _max, min_norm, max_norm):\n",
    "        if max_norm <= min_norm:\n",
    "            raise Exception('max_norm has to greater than min_norm')\n",
    "        if 0 in (_max - _min):\n",
    "            raise Exception('max and min are equal')\n",
    "        Data_norm = (max_norm - min_norm)*(Data - _min)/(_max - _min) + min_norm\n",
    "        return Data_norm\n",
    "    def meanNorm(self, Data, _min, _max, _mean):\n",
    "        if 0 in (_max - _min):\n",
    "            raise Exception('max and min are equal')\n",
    "        Data_norm = (Data - _mean)/(_max - _min)\n",
    "        return Data_norm\n",
    "    def standardization(self, Data, _mean, _std):\n",
    "        if 0 in _std:\n",
    "            raise Exception('std is equal 0')\n",
    "        Data_norm = (Data - _mean)/_std\n",
    "        return Data_norm\n",
    "    def SUL(self, Data, _ed):\n",
    "        if 0 in _ed:\n",
    "            raise Exception('ed is equal 0')\n",
    "        Data_norm = Data/_ed\n",
    "        return Data_norm\n",
    "    \n",
    "    def fit(self, Data):\n",
    "        if self.norm_type == 'minmaxNorm':\n",
    "            Data_norm = self.minmaxNorm(Data, self.Data_min, self.Data_max)\n",
    "        elif self.norm_type == 'rescale':\n",
    "            Data_norm = self.rescale(Data, self.Data_min, self.Data_max, self.Data_min_norm, self.Data_max_norm)\n",
    "        elif self.norm_type == 'meanNorm':\n",
    "            Data_norm = self.meanNorm(Data, self.Data_min, self.Data_max, self.Data_mean)\n",
    "        elif self.norm_type == 'standardization':\n",
    "            Data_norm = self.standardization(Data, self.Data_mean, self.Data_std)\n",
    "        elif self.norm_type == 'SUL':\n",
    "            Data_norm = self.SUL(Data, self.Data_ed)\n",
    "        return Data_norm\n",
    "    \n",
    "    def de_minmaxNorm(self, Data_norm, _min, _max):\n",
    "        Data = Data_norm*(_max - _min) + _min\n",
    "        return Data\n",
    "    def de_rescale(self, Data_norm, _min, _max, min_norm, max_norm):\n",
    "        if max_norm <= min_norm:\n",
    "            raise Exception('max_norm has to greater than min_norm')\n",
    "        Data = (_max - _min)*(Data_norm - min_norm)/(max_norm - min_norm) + _min\n",
    "        return Data\n",
    "    def de_meanNorm(self, Data_norm, _min, _max, _mean):\n",
    "        Data = Data_norm*(_max - _min) + _mean\n",
    "        return Data\n",
    "    def de_standardization(self, Data_norm, _mean, _std):\n",
    "        Data = Data_norm*_std + _mean\n",
    "        return Data\n",
    "    def de_SUL(self, Data_norm, _ed):\n",
    "        Data = Data_norm*_ed\n",
    "        return Data\n",
    "    \n",
    "    def transform(self, Data_norm):\n",
    "        if self.norm_type == 'minmaxNorm':\n",
    "            Data = self.de_minmaxNorm(Data_norm, self.Data_min, self.Data_max)\n",
    "        elif self.norm_type == 'rescale':\n",
    "            Data = self.de_rescale(Data_norm, self.Data_min, self.Data_max, self.Data_min_norm, self.Data_max_norm)\n",
    "        elif self.norm_type == 'meanNorm':\n",
    "            Data = self.de_meanNorm(Data_norm, self.Data_min, self.Data_max, self.Data_mean)\n",
    "        elif self.norm_type == 'standardization':\n",
    "            Data = self.de_standardization(Data_norm, self.Data_mean, self.Data_std)\n",
    "        elif self.norm_type == 'SUL':\n",
    "            Data = self.de_SUL(Data_norm, self.Data_ed)\n",
    "        return Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
